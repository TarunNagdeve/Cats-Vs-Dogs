{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0VUV3GM+8fGCSFiUTes2J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TarunNagdeve/Cats-Vs-Dogs/blob/master/Untitled72.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6qEM_Dl6n0m"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import layers\n",
        "from keras.applications import EfficientNetB0\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def compare_pos_tags(input_pos_tags, dataset_pos_tags):\n",
        "    # Get the lemmas of the POS tags\n",
        "    input_lemmas = [wordnet._morphy(tag) for _, tag in input_pos_tags]\n",
        "    dataset_lemmas = [wordnet._morphy(tag) for _, tag in dataset_pos_tags]\n",
        "\n",
        "    # Convert the lemmas into strings\n",
        "    input_lemma_string = ' '.join(input_lemmas)\n",
        "    dataset_lemma_string = ' '.join(dataset_lemmas)\n",
        "\n",
        "    # Vectorize the lemmas using TF-IDF\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    lemma_vectors = vectorizer.fit_transform([input_lemma_string, dataset_lemma_string])\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity = cosine_similarity(lemma_vectors)[0, 1]\n",
        "\n",
        "    return similarity\n"
      ],
      "metadata": {
        "id": "gQDiG51vfAyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "dataset = [...]  # Your dataset containing sentences\n",
        "# Perform any necessary preprocessing steps\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in dataset]\n",
        "\n",
        "# Perform POS tagging on the dataset sentences\n",
        "pos_tagged_sentences = [nltk.pos_tag(tokens) for tokens in tokenized_sentences]\n",
        "\n",
        "# Take the input sentence\n",
        "input_sentence = \"Your input sentence goes here\"\n",
        "input_tokens = nltk.word_tokenize(input_sentence)\n",
        "\n",
        "# Perform POS tagging on the input sentence\n",
        "input_pos_tags = nltk.pos_tag(input_tokens)\n",
        "\n",
        "# Compare POS tags and find the most similar sentence\n",
        "best_match = None\n",
        "best_similarity = float('-inf')\n",
        "\n",
        "for sentence, pos_tags in zip(dataset, pos_tagged_sentences):\n",
        "    similarity = compare_pos_tags(input_pos_tags, pos_tags)  # Implement your similarity metric\n",
        "    if similarity > best_similarity:\n",
        "        best_similarity = similarity\n",
        "        best_match = sentence\n",
        "\n",
        "# Print the most similar sentence\n",
        "print(\"Closest Match:\", best_match)\n"
      ],
      "metadata": {
        "id": "kj5x4LM7d_Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load the dataset or document collection\n",
        "data = pd.read_csv(\"your_dataset.csv\")  # Replace with your dataset file\n",
        "\n",
        "# Preprocess the text data (e.g., remove stopwords, tokenize, etc.)\n",
        "# ...\n",
        "\n",
        "# Create a document-term matrix using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "doc_term_matrix = vectorizer.fit_transform(data['text'])\n",
        "\n",
        "# Train the LDA model\n",
        "num_topics = 5  # Specify the number of topics\n",
        "lda_model = LatentDirichletAllocation(n_components=num_topics)\n",
        "lda_model.fit(doc_term_matrix)\n",
        "\n",
        "# Get the input sentence from the user\n",
        "input_sentence = \"Your input sentence goes here\"\n",
        "\n",
        "# Transform the input sentence into a document-term vector\n",
        "input_vector = vectorizer.transform([input_sentence])\n",
        "\n",
        "# Get the topic distribution for the input sentence\n",
        "input_topic_distribution = lda_model.transform(input_vector)\n",
        "\n",
        "# Find the closest matching sentence based on topic distribution similarity\n",
        "closest_sentence_index = input_topic_distribution.argmax()\n",
        "closest_sentence = data['text'][closest_sentence_index]\n",
        "\n",
        "# Print the closest matching sentence\n",
        "print(\"Closest Sentence:\", closest_sentence)\n"
      ],
      "metadata": {
        "id": "jCDpKIHNxRt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the stopwords corpus if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the dataframe or corpus\n",
        "data = pd.read_csv(\"your_dataset.csv\")  # Replace with your dataset file\n",
        "\n",
        "# Get the stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from the sentences in the dataframe column\n",
        "data['processed_text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
        "\n",
        "# Print the updated dataframe\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "t9gmIjETp9Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim import corpora, models\n",
        "\n",
        "# Load the dataset or document collection\n",
        "data = pd.read_csv(\"your_dataset.csv\")  # Replace with your dataset file\n",
        "\n",
        "# Preprocess the text data (e.g., remove stopwords, tokenize, etc.)\n",
        "# ...\n",
        "\n",
        "# Create a dictionary and document-term matrix\n",
        "documents = data['text'].tolist()  # Replace 'text' with the column containing the text data\n",
        "tokenized_docs = [doc.split() for doc in documents]\n",
        "dictionary = corpora.Dictionary(tokenized_docs)\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "# Train the LDA model\n",
        "num_topics = 5  # Specify the number of topics\n",
        "lda_model = models.LdaModel(doc_term_matrix, num_topics=num_topics, id2word=dictionary, passes=10)\n",
        "\n",
        "# Get the topics and their corresponding keywords\n",
        "topics = lda_model.print_topics(num_topics=num_topics)\n",
        "\n",
        "# Print the topics and their keywords\n",
        "for topic in topics:\n",
        "    print(topic)\n",
        "\n",
        "# Get the most dominant topic for each document\n",
        "doc_topics = []\n",
        "for doc in doc_term_matrix:\n",
        "    doc_topics.append(lda_model[doc])\n",
        "\n",
        "# Add the dominant topic to the dataframe\n",
        "data['dominant_topic'] = [max(topic, key=lambda x: x[1])[0] for topic in doc_topics]\n"
      ],
      "metadata": {
        "id": "4DgVJ0hqUab0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "\n",
        "# Load the pre-trained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Example pandas DataFrame with 'Description' column\n",
        "df = pd.DataFrame({'Description': ['Text example 1', 'Text example 2', 'Text example 3']})\n",
        "\n",
        "# Tokenize the input text data\n",
        "tokenized_inputs = tokenizer(list(df['Description']), padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
        "\n",
        "# Define the input tensors\n",
        "input_ids = tokenized_inputs['input_ids']\n",
        "attention_mask = tokenized_inputs['attention_mask']\n",
        "\n",
        "# Load the pre-trained BERT model\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Fine-tune the BERT model\n",
        "model.train()\n",
        "\n",
        "# Forward pass to obtain the embeddings\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "# Print the embeddings\n",
        "print(embeddings)\n"
      ],
      "metadata": {
        "id": "FwkMvZN_lKuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import logging\n",
        "\n",
        "# Disable W&B logging and warnings\n",
        "wandb_logger = logging.getLogger(\"wandb\")\n",
        "wandb_logger.setLevel(logging.ERROR)\n",
        "wandb.init = lambda *args, **kwargs: None\n",
        "wandb.login = lambda *args, **kwargs: None\n"
      ],
      "metadata": {
        "id": "Tdp7zSSZi-1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from wandb.sdk.wandb_login import prompt_api_key\n",
        "\n",
        "def no_op(*args, **kwargs):\n",
        "    pass\n",
        "\n",
        "wandb.login = no_op\n",
        "prompt_api_key = no_op\n",
        "\n",
        "wandb.ensure_configured()\n"
      ],
      "metadata": {
        "id": "OUkONh23djSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, TFBertForMaskedLM, TFTrainer, TFTrainingArguments\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = TFBertForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "# Example pandas DataFrame with text data in a 'text' column\n",
        "df = pd.DataFrame({'text': ['Text example 1', 'Text example 2', 'Text example 3']})\n",
        "\n",
        "# Tokenize the input text data\n",
        "tokenized_inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
        "\n",
        "# Define the input tensors\n",
        "input_ids = tokenized_inputs['input_ids']\n",
        "attention_mask = tokenized_inputs['attention_mask']\n",
        "\n",
        "# Define the training arguments with W&B disabled\n",
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./bert_finetuned',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,  # Adjust the number of training epochs as desired\n",
        "    per_device_train_batch_size=16,  # Adjust the batch size as desired\n",
        "    save_total_limit=2,  # Number of checkpoints to save\n",
        "    save_steps=500,  # Save a checkpoint every X steps\n",
        "    logging_steps=100,  # Log training metrics every X steps\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=1e-4,  # Adjust the learning rate as desired\n",
        "    disable_tqdm=True  # Disable W&B integration\n",
        ")\n",
        "\n",
        "# Create a Trainer instance for fine-tuning\n",
        "trainer = TFTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tf.data.Dataset.from_tensor_slices((input_ids, attention_mask))\n",
        ")\n",
        "\n",
        "# Fine-tune the BERT model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model('bert_finetuned')\n",
        "\n",
        "# Create the text embeddings using the fine-tuned model\n",
        "text = \"Your input text\"\n",
        "tokenized_text = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
        "\n",
        "# Define the input tensors for the text\n",
        "input_ids = tokenized_text['input_ids']\n",
        "attention_mask = tokenized_text['attention_mask']\n",
        "\n",
        "# Get the embeddings using the fine-tuned model\n",
        "outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "# Extract the embeddings from the model output\n",
        "embeddings = outputs.last_hidden_state.squeeze(axis=0)\n",
        "\n",
        "# Print the embeddings\n",
        "print(embeddings)\n"
      ],
      "metadata": {
        "id": "HyMNbZxMbDVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, TFBertForMaskedLM, TFTrainer, TFTrainingArguments\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = TFBertForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "# Example pandas DataFrame with text data in a 'text' column\n",
        "df = pd.DataFrame({'text': ['Text example 1', 'Text example 2', 'Text example 3']})\n",
        "\n",
        "# Tokenize the input text data\n",
        "tokenized_inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
        "\n",
        "# Define the input tensors\n",
        "input_ids = tokenized_inputs['input_ids']\n",
        "attention_mask = tokenized_inputs['attention_mask']\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./bert_finetuned',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,  # Adjust the number of training epochs as desired\n",
        "    per_device_train_batch_size=16,  # Adjust the batch size as desired\n",
        "    save_total_limit=2,  # Number of checkpoints to save\n",
        "    save_steps=500,  # Save a checkpoint every X steps\n",
        "    logging_steps=100,  # Log training metrics every X steps\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=1e-4,  # Adjust the learning rate as desired\n",
        ")\n",
        "\n",
        "# Create a Trainer instance for fine-tuning\n",
        "trainer = TFTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tf.data.Dataset.from_tensor_slices((input_ids, attention_mask))\n",
        ")\n",
        "\n",
        "# Fine-tune the BERT model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model('bert_finetuned')\n",
        "\n",
        "# Create the text embeddings using the fine-tuned model\n",
        "text = \"Your input text\"\n",
        "tokenized_text = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
        "\n",
        "# Define the input tensors for the text\n",
        "input_ids = tokenized_text['input_ids']\n",
        "attention_mask = tokenized_text['attention_mask']\n",
        "\n",
        "# Get the embeddings using the fine-tuned model\n",
        "outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "# Extract the embeddings from the model output\n",
        "embeddings = outputs.last_hidden_state.squeeze(axis=0)\n",
        "\n",
        "# Print the embeddings\n",
        "print(embeddings)\n"
      ],
      "metadata": {
        "id": "ehOVwfqaaGnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, TFBertForMaskedLM, TFTrainer, TFTrainingArguments\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = TFBertForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "# Example pandas DataFrame with text data in a 'text' column\n",
        "df = pd.DataFrame({'text': ['Text example 1', 'Text example 2', 'Text example 3']})\n",
        "\n",
        "# Tokenize the input text data\n",
        "tokenized_inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
        "\n",
        "# Define the input tensors\n",
        "input_ids = tokenized_inputs['input_ids']\n",
        "attention_mask = tokenized_inputs['attention_mask']\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./bert_finetuned',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,  # Adjust the number of training epochs as desired\n",
        "    per_device_train_batch_size=16,  # Adjust the batch size as desired\n",
        "    save_total_limit=2,  # Number of checkpoints to save\n",
        "    save_steps=500,  # Save a checkpoint every X steps\n",
        "    logging_steps=100,  # Log training metrics every X steps\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=1e-4,  # Adjust the learning rate as desired\n",
        ")\n",
        "\n",
        "# Create a Trainer instance for fine-tuning\n",
        "trainer = TFTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tf.data.Dataset.from_tensor_slices((input_ids, attention_mask))\n",
        ")\n",
        "\n",
        "# Fine-tune the BERT model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model('bert_finetuned')\n",
        "\n",
        "# Create the text embeddings using the fine-tuned model\n",
        "text = \"Your input text\"\n",
        "tokenized_text = tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
        "\n",
        "# Define the input tensors for the text\n",
        "input_ids = tokenized_text['input_ids']\n",
        "attention_mask = tokenized_text['attention_mask']\n",
        "\n",
        "# Get the embeddings using the fine-tuned model\n",
        "outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "# Extract the embeddings from the model output\n",
        "embeddings = outputs.last_hidden_state.squeeze(axis=0)\n",
        "\n",
        "# Print the embeddings\n",
        "print(embeddings)\n"
      ],
      "metadata": {
        "id": "9DAxjHcFZDi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pag_ssCHaFYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, TFBertForMaskedLM, TextDataset, DataCollatorForLanguageModeling, TFTrainer, TFTrainingArguments\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = TFBertForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "# Example pandas DataFrame with text data in a 'text' column\n",
        "df = pd.DataFrame({'text': ['Text example 1', 'Text example 2', 'Text example 3']})\n",
        "\n",
        "# Create a dataset from the DataFrame\n",
        "dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    data_frame=df,\n",
        "    text_column='text',\n",
        "    block_size=128  # Adjust the block size as desired\n",
        ")\n",
        "\n",
        "# Create a data collator for language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15  # Adjust the masking probability as desired\n",
        ")\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./bert_finetuned',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,  # Adjust the number of training epochs as desired\n",
        "    per_device_train_batch_size=16,  # Adjust the batch size as desired\n",
        "    save_total_limit=2,  # Number of checkpoints to save\n",
        "    save_steps=500,  # Save a checkpoint every X steps\n",
        "    logging_steps=100,  # Log training metrics every X steps\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=1e-4,  # Adjust the learning rate as desired\n",
        ")\n",
        "\n",
        "# Create a Trainer instance for fine-tuning\n",
        "trainer = TFTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "# Fine-tune the BERT model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model('bert_finetuned')\n",
        "\n",
        "# Create the text embeddings using the fine-tuned model\n",
        "text = \"Your input text\"\n",
        "inputs = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='tf')\n",
        "outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "\n",
        "# Extract the embeddings from the model output\n",
        "embeddings = outputs.last_hidden_state.squeeze(axis=0)\n",
        "\n",
        "# Print the embeddings\n",
        "print(embeddings)\n"
      ],
      "metadata": {
        "id": "aid6N36fYqtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zr_uW05GZClC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import BertTokenizerFast, TFBertModel\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "bert_model = TFBertModel.from_pretrained(model_name)\n",
        "\n",
        "# Prepare your dataset with descriptions\n",
        "descriptions = [...]  # Your dataset's description column as a list of sentences\n",
        "\n",
        "# Tokenize the descriptions\n",
        "encoded_inputs = tokenizer.batch_encode_plus(descriptions, padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "# Generate embeddings for the descriptions\n",
        "outputs = bert_model(encoded_inputs['input_ids'], attention_mask=encoded_inputs['attention_mask'])\n",
        "embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "# Convert the embeddings to a numpy array\n",
        "embeddings_np = np.array(embeddings)\n",
        "\n",
        "# Create a function to compute cosine similarity between two vectors\n",
        "def cosine_sim(a, b):\n",
        "    return cosine_similarity(a.reshape(1, -1), b.reshape(1, -1))[0][0]\n",
        "\n",
        "# Perform a nearest neighbor search for a given input text\n",
        "input_text = \"Your input text\"\n",
        "\n",
        "# Tokenize and encode the input text\n",
        "input_encoded = tokenizer.encode_plus(input_text, padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "# Get the embedding for the input text\n",
        "input_output = bert_model(input_encoded['input_ids'], attention_mask=input_encoded['attention_mask'])\n",
        "input_embedding = input_output.last_hidden_state[:, 0, :]\n",
        "\n",
        "# Compute cosine similarity between the input embedding and all descriptions' embeddings\n",
        "similarities = [cosine_sim(input_embedding.numpy(), embedding) for embedding in embeddings_np]\n",
        "\n",
        "# Sort the similarities in descending order and get the indices\n",
        "sorted_indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "# Retrieve the closest descriptions based on cosine similarity\n",
        "closest_descriptions = [descriptions[idx] for idx in sorted_indices[:5]]  # Get the top 5 closest descriptions\n",
        "\n",
        "# Print the closest descriptions\n",
        "for description in closest_descriptions:\n",
        "    print(description)\n"
      ],
      "metadata": {
        "id": "RtXmOSwQaBG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from transformers import BertTokenizerFast, TFBertModel\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "bert_model = TFBertModel.from_pretrained(model_name)\n",
        "\n",
        "# Prepare your dataset with descriptions\n",
        "descriptions = [...]  # Your dataset's description column as a list of sentences\n",
        "\n",
        "# Tokenize the descriptions\n",
        "encoded_inputs = tokenizer.batch_encode_plus(descriptions, padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "# Generate embeddings for the descriptions\n",
        "outputs = bert_model(encoded_inputs['input_ids'], attention_mask=encoded_inputs['attention_mask'])\n",
        "embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "# Convert the embeddings to a numpy array\n",
        "embeddings_np = np.array(embeddings)\n",
        "\n",
        "# Reshape the embeddings to 2D\n",
        "embeddings_2d = embeddings_np.reshape(embeddings_np.shape[0], -1)\n",
        "\n",
        "# Create an instance of NearestNeighbors\n",
        "nn = NearestNeighbors(n_neighbors=5, algorithm='auto')\n",
        "\n",
        "# Fit the NearestNeighbors on the embeddings\n",
        "nn.fit(embeddings_2d)\n",
        "\n",
        "# Perform a nearest neighbor search for a given input text\n",
        "input_text = \"Your input text\"\n",
        "\n",
        "# Tokenize and encode the input text\n",
        "input_encoded = tokenizer.encode_plus(input_text, padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "# Get the embedding for the input text\n",
        "input_output = bert_model(input_encoded['input_ids'], attention_mask=input_encoded['attention_mask'])\n",
        "input_embedding = input_output.last_hidden_state[:, 0, :]\n",
        "\n",
        "# Reshape the input embedding to 2D\n",
        "input_embedding_2d = input_embedding.numpy().reshape(1, -1)\n",
        "\n",
        "# Find the nearest neighbors to the input embedding\n",
        "distances, indices = nn.kneighbors(input_embedding_2d)\n",
        "\n",
        "# Retrieve the closest descriptions\n",
        "closest_descriptions = [descriptions[idx] for idx in indices[0]]\n",
        "\n",
        "# Print the closest descriptions\n",
        "for description in closest_descriptions:\n",
        "    print(description)\n"
      ],
      "metadata": {
        "id": "OYfWSVp6QxyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from transformers import BertTokenizerFast, TFBertModel\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "bert_model = TFBertModel.from_pretrained(model_name)\n",
        "\n",
        "# Prepare your dataset with descriptions\n",
        "descriptions = [...]  # Your dataset's description column as a list of sentences\n",
        "\n",
        "# Tokenize the descriptions\n",
        "encoded_inputs = tokenizer.batch_encode_plus(descriptions, padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "# Generate embeddings for the descriptions\n",
        "outputs = bert_model(encoded_inputs['input_ids'], attention_mask=encoded_inputs['attention_mask'])\n",
        "embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "# Convert the embeddings to a numpy array\n",
        "embeddings_np = np.array(embeddings)\n",
        "\n",
        "# Reshape the embeddings to 2D\n",
        "embeddings_2d = embeddings_np.reshape(embeddings_np.shape[0], -1)\n",
        "\n",
        "# Create an instance of NearestNeighbors\n",
        "nn = NearestNeighbors(n_neighbors=1, algorithm='auto')\n",
        "\n",
        "# Fit the NearestNeighbors on the embeddings\n",
        "nn.fit(embeddings_2d)\n",
        "\n",
        "# Perform a nearest neighbor search for a given input text\n",
        "input_text = \"Your input text\"\n",
        "\n",
        "# Tokenize and encode the input text\n",
        "input_encoded = tokenizer.encode_plus(input_text, padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "# Get the embedding for the input text\n",
        "input_output = bert_model(input_encoded['input_ids'], attention_mask=input_encoded['attention_mask'])\n",
        "input_embedding = input_output.last_hidden_state[:, 0, :]\n",
        "\n",
        "# Reshape the input embedding to 2D\n",
        "input_embedding_2d = input_embedding.numpy().reshape(1, -1)\n",
        "\n",
        "# Find the nearest neighbors to the input embedding\n",
        "distances, indices = nn.kneighbors(input_embedding_2d)\n",
        "\n",
        "# Retrieve the closest description\n",
        "closest_description = descriptions[indices[0][0]]\n",
        "\n",
        "# Print the closest description\n",
        "print(closest_description)\n"
      ],
      "metadata": {
        "id": "W4NR_2GzOsXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import LSHForest\n",
        "from transformers import BertTokenizerFast, TFBertModel\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "bert_model = TFBertModel.from_pretrained(model_name)\n",
        "\n",
        "# Prepare your dataset with descriptions\n",
        "descriptions = [...]  # Your dataset's description column as a list of sentences\n",
        "\n",
        "# Tokenize the descriptions\n",
        "encoded_inputs = tokenizer(descriptions, padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "# Generate embeddings for the descriptions\n",
        "outputs = bert_model(encoded_inputs.input_ids, attention_mask=encoded_inputs.attention_mask)\n",
        "embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "# Convert the embeddings to a numpy array\n",
        "embeddings_np = np.array(embeddings)\n",
        "\n",
        "# Create an instance of LSHForest\n",
        "lshf = LSHForest(n_estimators=10, n_candidates=100, random_state=42)\n",
        "\n",
        "# Fit the LSHForest on the embeddings\n",
        "lshf.fit(embeddings_np)\n",
        "\n",
        "# Perform a nearest neighbor search for a given input text\n",
        "input_text = \"Your input text\"\n",
        "\n",
        "# Tokenize and encode the input text\n",
        "input_encoded = tokenizer(input_text, padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "# Get the embedding for the input text\n",
        "input_output = bert_model(input_encoded.input_ids, attention_mask=input_encoded.attention_mask)\n",
        "input_embedding = input_output.last_hidden_state[:, 0, :]\n",
        "\n",
        "# Find the nearest neighbors to the input embedding\n",
        "_, indices = lshf.kneighbors([input_embedding], n_neighbors=1)  # Adjust the number of neighbors as desired\n",
        "\n",
        "# Retrieve the closest description\n",
        "closest_description = descriptions[indices[0][0]]\n",
        "\n",
        "# Print the closest description\n",
        "print(closest_description)\n"
      ],
      "metadata": {
        "id": "ytuyLGuY2SbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from transformers import BertTokenizerFast, TFBertModel\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "bert_model = TFBertModel.from_pretrained(model_name)\n",
        "\n",
        "# Prepare your dataset with descriptions\n",
        "descriptions = [...]  # Your dataset's description column as a list of sentences\n",
        "\n",
        "# Tokenize the descriptions\n",
        "encoded_inputs = tokenizer.batch_encode_plus(descriptions, padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "# Generate embeddings for the descriptions\n",
        "outputs = bert_model(encoded_inputs['input_ids'], attention_mask=encoded_inputs['attention_mask'])\n",
        "embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "# Convert the embeddings to a numpy array\n",
        "embeddings_np = np.array(embeddings)\n",
        "\n",
        "# Create an instance of NearestNeighbors\n",
        "nn = NearestNeighbors(n_neighbors=1, algorithm='auto')\n",
        "\n",
        "# Fit the NearestNeighbors on the embeddings\n",
        "nn.fit(embeddings_np)\n",
        "\n",
        "# Perform a nearest neighbor search for a given input text\n",
        "input_text = \"Your input text\"\n",
        "\n",
        "# Tokenize and encode the input text\n",
        "input_encoded = tokenizer.encode_plus(input_text, padding=True, truncation=True, return_tensors='tf')\n",
        "\n",
        "# Get the embedding for the input text\n",
        "input_output = bert_model(input_encoded['input_ids'], attention_mask=input_encoded['attention_mask'])\n",
        "input_embedding = input_output.last_hidden_state[:, 0, :]\n",
        "\n",
        "# Find the nearest neighbors to the input embedding\n",
        "distances, indices = nn.kneighbors([input_embedding])\n",
        "\n",
        "# Retrieve the closest description\n",
        "closest_description = descriptions[indices[0][0]]\n",
        "\n",
        "# Print the closest description\n",
        "print(closest_description)\n"
      ],
      "metadata": {
        "id": "L1QzdAgL2YoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "bert_model = TFBertModel.from_pretrained(model_name)\n",
        "\n",
        "# Prepare your dataset with descriptions\n",
        "descriptions = [...]  # Your dataset's description column as a list of sentences\n",
        "\n",
        "# Tokenize the descriptions\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "for description in descriptions:\n",
        "    encoded = tokenizer.encode_plus(\n",
        "        description,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,  # Adjust the max length as per your requirements\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "    input_ids.append(encoded['input_ids'][0])\n",
        "    attention_masks.append(encoded['attention_mask'][0])\n",
        "\n",
        "input_ids = tf.concat(input_ids, axis=0)\n",
        "attention_masks = tf.concat(attention_masks, axis=0)\n",
        "\n",
        "# Fine-tune the BERT model\n",
        "output = bert_model(input_ids=input_ids, attention_mask=attention_masks)\n",
        "embeddings = output.last_hidden_state[:, 0, :]\n",
        "\n",
        "# Print the embeddings\n",
        "print(embeddings)\n"
      ],
      "metadata": {
        "id": "2AO_OWJZwFCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import streamlit as st\n",
        "\n",
        "# Function to perform a long-running task asynchronously\n",
        "async def long_running_task():\n",
        "    total_iterations = 100\n",
        "    progress_bar = st.progress(0)\n",
        "    status_text = st.empty()\n",
        "\n",
        "    for i in range(total_iterations):\n",
        "        # Simulate computation or processing\n",
        "        await asyncio.sleep(0.1)\n",
        "\n",
        "        # Update progress bar and status text\n",
        "        progress = (i + 1) / total_iterations\n",
        "        progress_bar.progress(progress)\n",
        "        status_text.text(f\"Progress: {int(progress * 100)}%\")\n",
        "\n",
        "    # Task is complete\n",
        "    progress_bar.empty()\n",
        "    status_text.text(\"Task completed!\")\n",
        "\n",
        "# Run the long-running task asynchronously\n",
        "async def run_task():\n",
        "    await long_running_task()\n",
        "\n",
        "# Start the asyncio event loop\n",
        "loop = asyncio.new_event_loop()\n",
        "asyncio.set_event_loop(loop)\n",
        "task = loop.create_task(run_task())\n",
        "\n",
        "# Run the Streamlit app with the event loop running\n",
        "st.write(\"Long-running task started...\")\n",
        "loop.run_until_complete(task)\n",
        "st.write(\"Long-running task completed!\")\n"
      ],
      "metadata": {
        "id": "TJPKOF6Rftgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import streamlit as st\n",
        "\n",
        "# Function to perform a long-running task\n",
        "def long_running_task():\n",
        "    total_iterations = 100\n",
        "    progress_bar = st.progress(0)\n",
        "    status_text = st.empty()\n",
        "\n",
        "    for i in range(total_iterations):\n",
        "        # Perform computation or processing\n",
        "        time.sleep(0.1)\n",
        "\n",
        "        # Update progress bar and status text\n",
        "        progress = (i + 1) / total_iterations * 100\n",
        "        progress_bar.progress(progress)\n",
        "        status_text.text(f\"Progress: {int(progress)}%\")\n",
        "\n",
        "# Run the long-running task\n",
        "long_running_task()\n"
      ],
      "metadata": {
        "id": "xMIgSLYtbOZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from PIL import Image\n",
        "\n",
        "# Load the pre-trained model\n",
        "config_file = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
        "model_weights = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(config_file))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_weights)\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# Load and preprocess the image\n",
        "image_path = \"path_to_image.jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "image_tensor = torch.as_tensor(np.array(image), dtype=torch.uint8).permute(2, 0, 1)\n",
        "\n",
        "# Make predictions and extract features\n",
        "outputs = predictor(image_tensor)\n",
        "features = outputs[\"roi_features\"]\n",
        "\n",
        "# Convert features to a numpy array\n",
        "embedding = features.detach().numpy()\n",
        "\n",
        "# Print the shape of the embedding\n",
        "print(embedding.shape)\n"
      ],
      "metadata": {
        "id": "4MzWWShv89VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CD4xlg3kbLyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjPFy9cRbMgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# Define the URL of the webpage\n",
        "url = 'https://example.com'\n",
        "\n",
        "# Send a GET request to the webpage\n",
        "response = requests.get(url)\n",
        "\n",
        "# Create a BeautifulSoup object from the response content\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Define the section ID, class, div, and its class\n",
        "section_id = 'section-id'\n",
        "div_class = 'div-class'\n",
        "div_inner_class = 'div-inner-class'\n",
        "\n",
        "# Find the section element using its ID\n",
        "section_element = soup.find('section', {'id': section_id})\n",
        "\n",
        "if section_element:\n",
        "    # Find the div element inside the section using its class\n",
        "    div_element = section_element.find('div', {'class': div_class})\n",
        "\n",
        "    if div_element:\n",
        "        # Find the inner div element using its class\n",
        "        inner_div_element = div_element.find('div', {'class': div_inner_class})\n",
        "\n",
        "        if inner_div_element:\n",
        "            # Extract the desired information from the inner div element\n",
        "            data = inner_div_element.text.strip()\n",
        "            print(f\"Data: {data}\")\n",
        "        else:\n",
        "            print(f\"Inner div with class '{div_inner_class}' not found\")\n",
        "    else:\n",
        "        print(f\"Div with class '{div_class}' not found\")\n",
        "else:\n",
        "    print(f\"Section with ID '{section_id}' not found\")\n"
      ],
      "metadata": {
        "id": "r692Qz6I7CFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.firefox.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Configure Firefox options\n",
        "firefox_options = Options()\n",
        "firefox_options.headless = True  # Run Firefox in headless mode\n",
        "\n",
        "# Set path to geckodriver as per your configuration\n",
        "webdriver_path = '/path/to/geckodriver'\n",
        "\n",
        "# Create a new instance of FirefoxDriver\n",
        "driver = webdriver.Firefox(executable_path=webdriver_path, options=firefox_options)\n",
        "\n",
        "# Load the website\n",
        "url = 'https://www.example.com'\n",
        "driver.get(url)\n",
        "\n",
        "# Wait for the page to load and execute any required JavaScript\n",
        "# You can use WebDriverWait and expected_conditions for more control over waiting\n",
        "\n",
        "# Get the page source after JavaScript execution\n",
        "page_source = driver.page_source\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()\n",
        "\n",
        "# Parse the page source with BeautifulSoup\n",
        "soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "# Now you can use BeautifulSoup to extract the desired content\n",
        "# Find elements with the specified div and class attributes\n",
        "elements = soup.find_all('div', class_='your-class-name')\n",
        "\n",
        "for element in elements:\n",
        "    # Extract the text content or other attributes as needed\n",
        "    text = element.get_text()\n",
        "    print(text)\n"
      ],
      "metadata": {
        "id": "SXPZS3TJ-25m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://www.example.com'\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "}\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Find elements with the specified div and class attributes\n",
        "elements = soup.find_all('div', class_='your-class-name')\n",
        "\n",
        "for element in elements:\n",
        "    # Extract the text content or other attributes as needed\n",
        "    text = element.get_text()\n",
        "    print(text)\n"
      ],
      "metadata": {
        "id": "PSdBv06B89A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.firefox.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Configure Selenium WebDriver\n",
        "options = Options()\n",
        "options.headless = True  # Run Firefox in headless mode\n",
        "driver = webdriver.Firefox(options=options)\n",
        "\n",
        "# Specify the URL of the webpage you want to scrape\n",
        "url = \"https://pubchem.ncbi.nlm.nih.gov/#query=Palbociclib\"\n",
        "\n",
        "# Open the webpage using Selenium WebDriver\n",
        "driver.get(url)\n",
        "\n",
        "# Wait for the page to fully load (you may need to adjust the waiting time)\n",
        "driver.implicitly_wait(5)\n",
        "\n",
        "# Get the HTML content of the loaded webpage\n",
        "page_source = driver.page_source\n",
        "\n",
        "# Parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(page_source, \"html.parser\")\n",
        "\n",
        "# Find the desired section by ID and class\n",
        "section = soup.find(\"div\", id=\"Canonical-SMILES\", class_=\"pt-4\")\n",
        "\n",
        "# Find the Canonical SMILES within the section\n",
        "canonical_smiles = section.get_text(strip=True)\n",
        "\n",
        "# Print the Canonical SMILES\n",
        "print(\"Canonical SMILES:\", canonical_smiles)\n",
        "\n",
        "# Close the WebDriver\n",
        "driver.quit()\n"
      ],
      "metadata": {
        "id": "GSXTCBBtp5WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.firefox.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Configure Selenium WebDriver\n",
        "options = Options()\n",
        "options.headless = True  # Run Firefox in headless mode\n",
        "driver = webdriver.Firefox(options=options)\n",
        "\n",
        "# Specify the URL of the webpage you want to scrape\n",
        "url = \"https://pubchem.ncbi.nlm.nih.gov/#query=Palbociclib\"\n",
        "\n",
        "# Open the webpage using Selenium WebDriver\n",
        "driver.get(url)\n",
        "\n",
        "# Wait for the page to fully load (you may need to adjust the waiting time)\n",
        "driver.implicitly_wait(5)\n",
        "\n",
        "# Get the HTML content of the loaded webpage\n",
        "page_source = driver.page_source\n",
        "\n",
        "# Parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(page_source, \"html.parser\")\n",
        "\n",
        "# Find the desired subsection (2.1.4) based on its heading\n",
        "subsection_heading = soup.find(\"h2\", text=\"2.1.4\")\n",
        "\n",
        "# Find the next sibling div element containing the Canonical SMILES\n",
        "canonical_smiles = subsection_heading.find_next_sibling(\"div\").get_text(strip=True)\n",
        "\n",
        "# Print the Canonical SMILES\n",
        "print(\"Canonical SMILES:\", canonical_smiles)\n",
        "\n",
        "# Close the WebDriver\n",
        "driver.quit()\n"
      ],
      "metadata": {
        "id": "S0ZkNIgvoOY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.firefox.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Configure Selenium WebDriver\n",
        "options = Options()\n",
        "options.headless = True  # Run Firefox in headless mode\n",
        "driver = webdriver.Firefox(options=options)\n",
        "\n",
        "# Specify the URL of the webpage you want to scrape\n",
        "url = \"https://pubchem.ncbi.nlm.nih.gov/#query=Palbociclib\"\n",
        "\n",
        "# Open the webpage using Selenium WebDriver\n",
        "driver.get(url)\n",
        "\n",
        "# Wait for the page to fully load (you may need to adjust the waiting time)\n",
        "driver.implicitly_wait(5)\n",
        "\n",
        "# Get the HTML content of the loaded webpage\n",
        "page_source = driver.page_source\n",
        "\n",
        "# Parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(page_source, \"html.parser\")\n",
        "\n",
        "# Extract the desired information from the webpage\n",
        "# Customize the code below based on the specific elements and data you want to scrape\n",
        "info_elements = soup.find_all(\"div\", class_=\"info-content\")\n",
        "for info_element in info_elements:\n",
        "    # Extract and print the text content of each info element\n",
        "    info_text = info_element.get_text()\n",
        "    print(info_text)\n",
        "\n",
        "# Close the WebDriver\n",
        "driver.quit()\n"
      ],
      "metadata": {
        "id": "-EH7nVbmDTnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Specify the base URL of the website\n",
        "base_url = \"https://www.example.com\"\n",
        "\n",
        "# Specify the search query parameter\n",
        "search_query = \"your_search_query\"\n",
        "\n",
        "# Construct the complete URL with the search query\n",
        "url = f\"{base_url}/search?q={search_query}\"\n",
        "\n",
        "# Send a GET request to the search URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content of the search results page using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Extract data from the search results using CSS selectors or other methods\n",
        "    # Example: Get all the search result titles and links\n",
        "    search_results = soup.select(\".result-item\")\n",
        "    for result in search_results:\n",
        "        title = result.find(\"h2\").get_text()\n",
        "        link = result.find(\"a\")[\"href\"]\n",
        "        print(\"Title:\", title)\n",
        "        print(\"Link:\", link)\n",
        "\n",
        "        # Follow the link and scrape more data if needed\n",
        "\n",
        "else:\n",
        "    print(\"Failed to retrieve the search results. Status code:\", response.status_code)\n"
      ],
      "metadata": {
        "id": "9lNFSsCeATPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Specify the URL of the website you want to scrape\n",
        "url = \"https://www.example.com\"\n",
        "\n",
        "# Send a GET request to the website\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content of the website using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Extract data from the website using CSS selectors or other methods\n",
        "    # Example: Get the title of the webpage\n",
        "    title = soup.find(\"title\").get_text()\n",
        "    print(\"Title:\", title)\n",
        "\n",
        "    # Example: Get all the links on the webpage\n",
        "    links = soup.find_all(\"a\")\n",
        "    for link in links:\n",
        "        href = link.get(\"href\")\n",
        "        text = link.get_text()\n",
        "        print(\"Link:\", text, \"->\", href)\n",
        "\n",
        "    # Example: Get text from specific HTML elements\n",
        "    # Use appropriate CSS selectors to target the desired elements\n",
        "    data_elements = soup.select(\".data-class\")\n",
        "    for element in data_elements:\n",
        "        data = element.get_text()\n",
        "        print(\"Data:\", data)\n",
        "else:\n",
        "    print(\"Failed to retrieve the website data. Status code:\", response.status_code)\n"
      ],
      "metadata": {
        "id": "35vm4wLb_Vxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "\n",
        "# Load and preprocess the dataset of images\n",
        "image_paths = [...]  # List of image file paths\n",
        "# Preprocess the images as necessary (e.g., resize, normalize, etc.)\n",
        "\n",
        "# Load a pre-trained CNN model for feature extraction\n",
        "cnn_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "# Extract image embeddings or feature vectors\n",
        "embeddings = []\n",
        "for image_path in image_paths:\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert image to RGB format\n",
        "    image = cv2.resize(image, (224, 224))  # Resize image to match input size of the CNN model\n",
        "    image = preprocess_input(image)  # Preprocess the image\n",
        "    embedding = cnn_model.predict(np.expand_dims(image, axis=0))  # Extract the feature vector\n",
        "    embeddings.append(embedding.flatten())\n",
        "\n",
        "# Convert the list of embeddings to a numpy array\n",
        "embeddings = np.array(embeddings)\n",
        "\n",
        "# Train the K-means clustering model\n",
        "k = 5  # Number of clusters\n",
        "kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "kmeans.fit(embeddings)\n",
        "\n",
        "# Get the cluster labels for each image\n",
        "cluster_labels = kmeans.labels_\n",
        "\n",
        "# Print the cluster labels for each image\n",
        "for i, label in enumerate(cluster_labels):\n",
        "    print(f\"Image {i}: Cluster {label}\")\n",
        "\n",
        "# Perform further analysis or visualization with the obtained cluster labels\n"
      ],
      "metadata": {
        "id": "jXj9S5aaRArH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the blank image\n",
        "blank_image = np.zeros((500, 500, 3), dtype=np.uint8)\n",
        "\n",
        "# List of cropped images, their location coordinates (x, y), and corresponding labels\n",
        "cropped_images = [...]  # List of cropped images\n",
        "coordinates = [...]  # List of (x, y) location coordinates for each cropped image\n",
        "labels = [...]  # List of labels/classes for each cropped image\n",
        "\n",
        "# Define colors for different classes\n",
        "class_colors = [(0, 0, 255), (0, 255, 0), (255, 0, 0)]  # Example colors for 3 classes\n",
        "\n",
        "# Iterate over the cropped images, coordinates, and labels\n",
        "for image, (x, y), label in zip(cropped_images, coordinates, labels):\n",
        "    # Get the dimensions of the cropped image\n",
        "    height, width, _ = image.shape\n",
        "\n",
        "    # Calculate the end coordinates for plotting the image\n",
        "    x_end = x + width\n",
        "    y_end = y + height\n",
        "\n",
        "    # Get the color for the current class\n",
        "    color = class_colors[label]\n",
        "\n",
        "    # Draw a rectangle with the assigned color on the blank image\n",
        "    cv2.rectangle(blank_image, (x, y), (x_end, y_end), color, thickness=2)\n",
        "\n",
        "    # Plot the cropped image onto the blank image\n",
        "    blank_image[y:y_end, x:x_end] = image\n",
        "\n",
        "# Display the resulting image\n",
        "cv2.imshow(\"Image with Cropped Images\", blank_image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "K9TjRxeSDRJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the feature matrix X\n",
        "# X is a 2D array where each row represents a sample (pixel) and each column represents a feature (RGB values)\n",
        "# Replace X with your actual feature matrix\n",
        "\n",
        "# Initialize empty lists to store values of k and corresponding inertia (sum of squared distances to the nearest centroid)\n",
        "k_values = []\n",
        "inertias = []\n",
        "\n",
        "# Set the range of k values to explore\n",
        "k_range = range(1, 11)\n",
        "\n",
        "# Perform K-means clustering for each k value and calculate inertia\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X)\n",
        "    inertia = kmeans.inertia_\n",
        "\n",
        "    k_values.append(k)\n",
        "    inertias.append(inertia)\n",
        "\n",
        "# Plot the inertia values for different k values\n",
        "plt.plot(k_values, inertias, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method - Inertia vs. Number of Clusters')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "v62pmypFT0V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def main():\n",
        "    st.title(\"Drawable Canvas Example\")\n",
        "\n",
        "    # Create a drawable canvas\n",
        "    canvas_result = st.drawable_canvas(\n",
        "        stroke_width=10,\n",
        "        stroke_color=\"#ffffff\",\n",
        "        background_color=\"#000000\",\n",
        "        width=500,\n",
        "        height=500,\n",
        "        key=\"canvas\"\n",
        "    )\n",
        "\n",
        "    if canvas_result.image_data is not None:\n",
        "        # Convert the image data to a PIL Image\n",
        "        image = Image.fromarray(canvas_result.image_data.astype(np.uint8))\n",
        "\n",
        "        # Display the image\n",
        "        st.image(image, caption=\"Canvas Image\")\n",
        "\n",
        "        # Get the real coordinates of the drawn lines\n",
        "        real_coordinates = np.array(canvas_result.json_data[\"objects\"][0][\"points\"])\n",
        "\n",
        "        # Scale the coordinates to the canvas size\n",
        "        canvas_width = canvas_result.image.shape[1]\n",
        "        canvas_height = canvas_result.image.shape[0]\n",
        "        scaled_coordinates = real_coordinates * np.array([canvas_width, canvas_height])\n",
        "\n",
        "        # Display the real coordinates\n",
        "        st.write(\"### Real Coordinates\")\n",
        "        for i, coord in enumerate(scaled_coordinates):\n",
        "            st.write(f\"Point {i + 1}: ({coord[0]}, {coord[1]})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "VKWDto0kNM7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "def mark_points(image, points):\n",
        "    img = Image.open(image)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    for point in points:\n",
        "        draw.rectangle([point[0] - 2, point[1] - 2, point[0] + 2, point[1] + 2], outline='red')\n",
        "\n",
        "    return img\n",
        "\n",
        "def main():\n",
        "    st.title(\"Mark Extreme Points on an Image\")\n",
        "    uploaded_file = st.file_uploader(\"Choose an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        image = Image.open(uploaded_file)\n",
        "        st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
        "\n",
        "        points = []\n",
        "        marked_image = None\n",
        "\n",
        "        if st.button(\"Mark Points\"):\n",
        "            marked_image = mark_points(uploaded_file, points)\n",
        "            st.image(marked_image, caption=\"Marked Image\", use_column_width=True)\n",
        "\n",
        "        click_coordinates = st.button(\"Click on Image to Add Point\")\n",
        "\n",
        "        if click_coordinates:\n",
        "            st.markdown(\n",
        "                \"\"\"\n",
        "                <style>\n",
        "                canvas { border: 1px solid black }\n",
        "                </style>\n",
        "                \"\"\"\n",
        "            )\n",
        "\n",
        "            st.write(\n",
        "                \"\"\"\n",
        "                <script>\n",
        "                const canvas = document.getElementsByTagName('canvas')[0];\n",
        "                const rect = canvas.getBoundingClientRect();\n",
        "                canvas.addEventListener('click', function(event) {\n",
        "                    const x = event.clientX - rect.left;\n",
        "                    const y = event.clientY - rect.top;\n",
        "                    const data = { x, y };\n",
        "                    const jsonString = JSON.stringify(data);\n",
        "                    Streamlit.connection.sendMessage(jsonString);\n",
        "                });\n",
        "                </script>\n",
        "                \"\"\"\n",
        "            )\n",
        "\n",
        "            if \"data\" not in st.session_state:\n",
        "                st.session_state.data = []\n",
        "\n",
        "            if st.session_state.data:\n",
        "                data = st.session_state.data\n",
        "                points.append((data[\"x\"], data[\"y\"]))\n",
        "                del st.session_state.data\n",
        "\n",
        "        if len(points) > 0:\n",
        "            st.write(\"### Marked Points\")\n",
        "            for i, point in enumerate(points):\n",
        "                st.write(f\"Point {i + 1}: ({point[0]}, {point[1]})\")\n",
        "\n",
        "    else:\n",
        "        st.warning(\"Please upload an image file.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "fBkUv_fcAc3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "def mark_points(image, points):\n",
        "    img = Image.open(image)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    for point in points:\n",
        "        draw.rectangle([point[0] - 2, point[1] - 2, point[0] + 2, point[1] + 2], outline='red')\n",
        "\n",
        "    return img\n",
        "\n",
        "def main():\n",
        "    st.title(\"Mark Extreme Points on an Image\")\n",
        "    uploaded_file = st.file_uploader(\"Choose an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        image = Image.open(uploaded_file)\n",
        "        st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
        "\n",
        "        points = []\n",
        "        marked_image = None\n",
        "\n",
        "        if st.button(\"Mark Points\"):\n",
        "            marked_image = mark_points(uploaded_file, points)\n",
        "            st.image(marked_image, caption=\"Marked Image\", use_column_width=True)\n",
        "\n",
        "        click_coordinates = st.button(\"Click on Image to Add Point\")\n",
        "\n",
        "        if click_coordinates:\n",
        "            clicked_x = st.session_state.mouse_click_x\n",
        "            clicked_y = st.session_state.mouse_click_y\n",
        "\n",
        "            if clicked_x and clicked_y:\n",
        "                points.append((clicked_x, clicked_y))\n",
        "\n",
        "        if len(points) > 0:\n",
        "            st.write(\"### Marked Points\")\n",
        "            for i, point in enumerate(points):\n",
        "                st.write(f\"Point {i + 1}: ({point[0]}, {point[1]})\")\n",
        "\n",
        "    else:\n",
        "        st.warning(\"Please upload an image file.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ZmSkVeN-_NJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "def mark_points(image, points):\n",
        "    img = Image.open(image)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    for point in points:\n",
        "        draw.rectangle([point[0] - 2, point[1] - 2, point[0] + 2, point[1] + 2], outline='red')\n",
        "\n",
        "    return img\n",
        "\n",
        "def main():\n",
        "    st.title(\"Mark Extreme Points on an Image\")\n",
        "    uploaded_file = st.file_uploader(\"Choose an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        image = Image.open(uploaded_file)\n",
        "        st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
        "\n",
        "        points = []\n",
        "        marked_image = None\n",
        "\n",
        "        if st.button(\"Mark Points\"):\n",
        "            marked_image = mark_points(uploaded_file, points)\n",
        "\n",
        "        col1, col2 = st.beta_columns(2)\n",
        "\n",
        "        with col1:\n",
        "            x = st.number_input(\"X Coordinate\", value=0)\n",
        "        with col2:\n",
        "            y = st.number_input(\"Y Coordinate\", value=0)\n",
        "\n",
        "        if st.button(\"Add Point\"):\n",
        "            points.append((x, y))\n",
        "\n",
        "        if marked_image is not None:\n",
        "            st.image(marked_image, caption=\"Marked Image\", use_column_width=True)\n",
        "\n",
        "        if len(points) > 0:\n",
        "            st.write(\"### Marked Points\")\n",
        "            for i, point in enumerate(points):\n",
        "                st.write(f\"Point {i + 1}: ({point[0]}, {point[1]})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "lZ-N04Ym9nTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "\n",
        "def main():\n",
        "    uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        # Read the uploaded image\n",
        "        image_data = uploaded_file.getvalue()\n",
        "        image = np.array(bytearray(image_data), dtype=np.uint8)\n",
        "        image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "\n",
        "        # Display the original image\n",
        "        st.image(image, channels=\"BGR\")\n",
        "\n",
        "        # Apply st_cropper to get the cropped image\n",
        "        cropped_image = st_cropper(image)\n",
        "\n",
        "        # Display the cropped image\n",
        "        if cropped_image is not None:\n",
        "            st.image(cropped_image, channels=\"BGR\")\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def st_cropper(image):\n",
        "    # Create a copy of the original image\n",
        "    image_copy = np.copy(image)\n",
        "\n",
        "    # Set the default cropped image to None\n",
        "    cropped_image = None\n",
        "\n",
        "    # Show the image using st.image\n",
        "    st.image(image_copy, channels=\"BGR\", use_column_width=True)\n",
        "\n",
        "    # Get the cropped coordinates using st_cropper\n",
        "    cropped_coords = st_cropper_widget(image_copy)\n",
        "\n",
        "    # Check if cropping is done\n",
        "    if cropped_coords is not None:\n",
        "        # Extract the coordinates\n",
        "        x1, y1, x2, y2 = cropped_coords\n",
        "\n",
        "        # Perform cropping\n",
        "        cropped_image = image_copy[y1:y2, x1:x2]\n",
        "\n",
        "    return cropped_image\n",
        "\n",
        "def st_cropper_widget(image):\n",
        "    # Generate a unique hash for the image\n",
        "    image_hash = st.hash(image.tobytes())\n",
        "\n",
        "    # Create a unique key for the widget\n",
        "    widget_key = f\"cropper_{image_hash}\"\n",
        "\n",
        "    # Get the cropped coordinates using st_cropper\n",
        "    cropped_coords = st_cropper(image, key=widget_key)\n",
        "\n",
        "    return cropped_coords\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ZzTfBSWixJL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "\n",
        "# Function to handle mouse events\n",
        "def mouse_callback(event, x, y, flags, param):\n",
        "    if event == cv2.EVENT_LBUTTONDOWN:\n",
        "        # Set the initial coordinates of the bounding box\n",
        "        param['x1'], param['y1'] = x, y\n",
        "        param['x2'], param['y2'] = x, y\n",
        "    elif event == cv2.EVENT_LBUTTONUP:\n",
        "        # Set the final coordinates of the bounding box\n",
        "        param['x2'], param['y2'] = x, y\n",
        "        # Display the cropped image\n",
        "        cropped_image = param['image'][param['y1']:param['y2'], param['x1']:param['x2']]\n",
        "        cv2.imshow('Cropped Image', cropped_image)\n",
        "        cv2.waitKey(0)\n",
        "\n",
        "def main():\n",
        "    uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        # Read the uploaded image\n",
        "        image_data = uploaded_file.getvalue()\n",
        "        image = cv2.imdecode(np.frombuffer(image_data, np.uint8), cv2.IMREAD_COLOR)\n",
        "\n",
        "        # Create a dictionary to store the coordinates\n",
        "        bbox_coords = {'x1': -1, 'y1': -1, 'x2': -1, 'y2': -1, 'image': image}\n",
        "\n",
        "        # Display the image\n",
        "        st.image(image)\n",
        "\n",
        "        # Set up the mouse callback function\n",
        "        cv2.namedWindow('Image')\n",
        "        cv2.setMouseCallback('Image', mouse_callback, bbox_coords)\n",
        "\n",
        "        # Wait for the user to select the bounding box\n",
        "        while True:\n",
        "            cv2.imshow('Image', image)\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "\n",
        "        # Close all windows\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "h9uU81B2hukr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import cv2\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    image = cv2.imdecode(np.frombuffer(uploaded_file.read(), np.uint8), cv2.IMREAD_COLOR)\n",
        "    st.image(image)\n",
        "\n",
        "    bbox_x = st.slider(\"Bounding Box X\", 0, image.shape[1], 0)\n",
        "    bbox_y = st.slider(\"Bounding Box Y\", 0, image.shape[0], 0)\n",
        "    bbox_width = st.slider(\"Bounding Box Width\", 0, image.shape[1] - bbox_x, 100)\n",
        "    bbox_height = st.slider(\"Bounding Box Height\", 0, image.shape[0] - bbox_y, 100)\n",
        "\n",
        "    if st.button(\"Crop\"):\n",
        "        cropped_image = image[bbox_y:bbox_y+bbox_height, bbox_x:bbox_x+bbox_width]\n",
        "        st.image(cropped_image)\n"
      ],
      "metadata": {
        "id": "bnna87iQg7ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "# Set page title\n",
        "st.set_page_config(page_title=\"Image Coordinate Extraction\")\n",
        "\n",
        "# Upload image\n",
        "uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "# Check if image is uploaded\n",
        "if uploaded_file is not None:\n",
        "    # Open the uploaded image\n",
        "    image = Image.open(uploaded_file)\n",
        "\n",
        "    # Display the uploaded image\n",
        "    st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
        "\n",
        "    # Get the image size\n",
        "    image_width, image_height = image.size\n",
        "\n",
        "    # Create a drawing object\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    # Track clicked coordinates\n",
        "    clicked_coordinates = []\n",
        "\n",
        "    # Mouse click event handler\n",
        "    def on_mouse_click(x, y, button, _):\n",
        "        if button == \"left\":\n",
        "            # Save the clicked coordinates\n",
        "            clicked_coordinates.append((x, y))\n",
        "\n",
        "            # Draw a point at the clicked coordinates\n",
        "            draw.point((x, y), fill=\"red\")\n",
        "\n",
        "            # Update the displayed image with the marked point\n",
        "            st.image(image, caption=\"Marked Image\", use_column_width=True)\n",
        "\n",
        "            # Print the clicked coordinates\n",
        "            st.write(f\"Clicked Coordinate: ({x}, {y})\")\n",
        "\n",
        "    # Register the mouse click event handler\n",
        "    st.image(image, caption=\"Click on extreme points\", use_column_width=True, format=\"PNG\", output_format=\"PNG\", use_column_width=True)\n",
        "    st_canvas = st.image(image, use_column_width=True, format='PNG')\n",
        "    st_canvas._get_coordinates = True\n",
        "    st_canvas._recorded_coords = []\n",
        "    st_canvas.add_mouse_callback(on_mouse_click)\n"
      ],
      "metadata": {
        "id": "h6p6rMXsPdTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import streamlit as st\n",
        "\n",
        "# Function to read the uploaded image using OpenCV\n",
        "def read_image(uploaded_image):\n",
        "    # Convert the uploaded image to numpy array\n",
        "    image_array = np.asarray(uploaded_image)\n",
        "\n",
        "    # Convert the image to BGR format (required by OpenCV)\n",
        "    image_bgr = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    return image_bgr\n",
        "\n",
        "# Main function to run the Streamlit app\n",
        "def main():\n",
        "    # Title and file upload section\n",
        "    st.title(\"Image Processing\")\n",
        "    uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "    # Check if an image is uploaded\n",
        "    if uploaded_file is not None:\n",
        "        # Read the uploaded image using OpenCV\n",
        "        image = read_image(uploaded_file)\n",
        "\n",
        "        # Display the uploaded image\n",
        "        st.image(image, channels=\"BGR\")\n",
        "\n",
        "        # Mouse callback function\n",
        "        def mouse_callback(event, x, y, flags, param):\n",
        "            if event == cv2.EVENT_LBUTTONDOWN:\n",
        "                print(f\"Clicked at x={x}, y={y}\")\n",
        "\n",
        "        # Set the mouse callback\n",
        "        cv2.namedWindow(\"Image\")\n",
        "        cv2.setMouseCallback(\"Image\", mouse_callback)\n",
        "\n",
        "        # Wait for any key to be pressed\n",
        "        cv2.waitKey(0)\n",
        "\n",
        "        # Close the OpenCV windows\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "UXSTQYcyqbLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import cv2\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "def main():\n",
        "    # Global variables\n",
        "    image = None\n",
        "    click_coordinates = []\n",
        "\n",
        "    st.title(\"Extreme Coordinates Extraction\")\n",
        "\n",
        "    uploaded_file = st.file_uploader(\"Upload an image\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        # Read the image file\n",
        "        file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)\n",
        "        image = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)\n",
        "\n",
        "        # Display the image\n",
        "        st.image(image, channels=\"BGR\", use_column_width=True)\n",
        "\n",
        "        # Button to start capturing coordinates\n",
        "        capture_button = st.button(\"Capture Coordinates\")\n",
        "\n",
        "        if capture_button:\n",
        "            # Create a plotly figure for displaying the image and capturing clicks\n",
        "            fig = go.Figure()\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=[],\n",
        "                    y=[],\n",
        "                    mode=\"markers\",\n",
        "                    marker=dict(size=10, color=\"red\")\n",
        "                )\n",
        "            )\n",
        "\n",
        "            fig.update_layout(\n",
        "                images=[\n",
        "                    go.layout.Image(\n",
        "                        source=image,\n",
        "                        xref=\"x\",\n",
        "                        yref=\"y\",\n",
        "                        x=0,\n",
        "                        y=0,\n",
        "                        sizex=image.shape[1],\n",
        "                        sizey=image.shape[0],\n",
        "                        sizing=\"stretch\",\n",
        "                        layer=\"below\"\n",
        "                    )\n",
        "                ],\n",
        "                showlegend=False,\n",
        "                width=image.shape[1],\n",
        "                height=image.shape[0],\n",
        "                margin=dict(l=0, r=0, t=0, b=0)\n",
        "            )\n",
        "\n",
        "            # Render the plotly figure\n",
        "            plotly_figure = st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "            # Wait for user clicks\n",
        "            while plotly_figure.active:\n",
        "                # Get the click coordinates from the plotly figure\n",
        "                if st.button(\"Capture Point\"):\n",
        "                    click_data = plotly_figure.json_data[\"props\"][\"figure\"][\"data\"][0][\"x\"], \\\n",
        "                                 plotly_figure.json_data[\"props\"][\"figure\"][\"data\"][0][\"y\"]\n",
        "                    click_coordinates.append(click_data)\n",
        "\n",
        "            # Display the captured coordinates\n",
        "            if click_coordinates:\n",
        "                st.subheader(\"Captured Coordinates:\")\n",
        "                for i, (x, y) in enumerate(click_coordinates):\n",
        "                    st.write(f\"Point {i + 1}: x={x}, y={y}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "AhnwbBDPlsL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CSu4c4qtqAL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def main():\n",
        "    # Global variables\n",
        "    image = None\n",
        "    click_coordinates = []\n",
        "\n",
        "    def click_event(event):\n",
        "        nonlocal click_coordinates\n",
        "\n",
        "        if event.button == 1:  # Left mouse button\n",
        "            x, y = int(event.xdata), int(event.ydata)\n",
        "            click_coordinates.append((x, y))\n",
        "\n",
        "    # Upload image\n",
        "    st.title(\"Extreme Coordinates Extraction\")\n",
        "    uploaded_file = st.file_uploader(\"Upload an image\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        # Read the image\n",
        "        image = plt.imread(uploaded_file)\n",
        "\n",
        "        # Create a new figure and display the image\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.imshow(image)\n",
        "\n",
        "        # Register the mouse click event\n",
        "        fig.canvas.mpl_connect(\"button_press_event\", click_event)\n",
        "\n",
        "        # Display the image using Streamlit\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Convert the click coordinates to numpy array\n",
        "        click_coordinates = np.array(click_coordinates)\n",
        "\n",
        "        # Print the extreme coordinates\n",
        "        st.subheader(\"Extreme Coordinates:\")\n",
        "        for coordinate in click_coordinates:\n",
        "            x, y = coordinate\n",
        "            st.write(f\"x: {x}, y: {y}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "a4xQUlKyjLgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Global variables\n",
        "image = None\n",
        "click_coordinates = []\n",
        "\n",
        "def click_event(event):\n",
        "    global click_coordinates\n",
        "\n",
        "    if event.button == 1:  # Left mouse button\n",
        "        x, y = int(event.xdata), int(event.ydata)\n",
        "        click_coordinates.append((x, y))\n",
        "\n",
        "def main():\n",
        "    global image, click_coordinates\n",
        "\n",
        "    # Upload image\n",
        "    st.title(\"Extreme Coordinates Extraction\")\n",
        "    uploaded_file = st.file_uploader(\"Upload an image\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        # Read the image\n",
        "        image = plt.imread(uploaded_file)\n",
        "\n",
        "        # Create a new figure and display the image\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.imshow(image)\n",
        "\n",
        "        # Register the mouse click event\n",
        "        fig.canvas.mpl_connect(\"button_press_event\", click_event)\n",
        "\n",
        "        # Display the image using Streamlit\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # Convert the click coordinates to numpy array\n",
        "        click_coordinates = np.array(click_coordinates)\n",
        "\n",
        "        # Print the extreme coordinates\n",
        "        st.subheader(\"Extreme Coordinates:\")\n",
        "        for coordinate in click_coordinates:\n",
        "            x, y = coordinate\n",
        "            st.write(f\"x: {x}, y: {y}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "MztnuWDFhWwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9fKeC0j8jJEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Global variables\n",
        "image = None\n",
        "click_coordinates = []\n",
        "\n",
        "def click_event(event):\n",
        "    global click_coordinates\n",
        "\n",
        "    if event.button == 1:  # Left mouse button\n",
        "        x, y = int(event.xdata), int(event.ydata)\n",
        "        click_coordinates.append((x, y))\n",
        "\n",
        "def main():\n",
        "    global image, click_coordinates\n",
        "\n",
        "    # Load the image\n",
        "    image_path = \"path/to/your/image.jpg\"\n",
        "    image = plt.imread(image_path)\n",
        "\n",
        "    # Create a new figure and display the image\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(image)\n",
        "\n",
        "    # Register the mouse click event\n",
        "    fig.canvas.mpl_connect(\"button_press_event\", click_event)\n",
        "\n",
        "    # Display the image and wait for user interaction\n",
        "    plt.show()\n",
        "\n",
        "    # Convert the click coordinates to numpy array\n",
        "    click_coordinates = np.array(click_coordinates)\n",
        "\n",
        "    # Print the extreme coordinates\n",
        "    print(\"Extreme Coordinates:\")\n",
        "    for coordinate in click_coordinates:\n",
        "        x, y = coordinate\n",
        "        print(f\"x: {x}, y: {y}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "hUswITcffMnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Global variables\n",
        "image = None\n",
        "click_coordinates = []\n",
        "\n",
        "def click_event(event):\n",
        "    global click_coordinates\n",
        "\n",
        "    if event.button == 1:  # Left mouse button\n",
        "        x, y = event.x, event.y\n",
        "        click_coordinates.append((x, y))\n",
        "\n",
        "def main():\n",
        "    global image, click_coordinates\n",
        "\n",
        "    # Load the image\n",
        "    image_path = \"path/to/your/image.jpg\"\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # Create a copy of the image for display\n",
        "    image_display = image.copy()\n",
        "\n",
        "    # Display the image\n",
        "    image_display.show()\n",
        "\n",
        "    # Get the image dimensions\n",
        "    width, height = image_display.size\n",
        "\n",
        "    # Create a new image with mouse events\n",
        "    image_with_events = Image.new(\"RGB\", (width, height))\n",
        "    image_with_events.paste(image, (0, 0))\n",
        "\n",
        "    # Register the mouse click event\n",
        "    image_with_events.show(title=\"Click on Extreme Coordinates\")\n",
        "    image_with_events.canvas.mpl_connect(\"button_press_event\", click_event)\n",
        "\n",
        "    # Wait for user to close the image\n",
        "    input(\"Press Enter after selecting coordinates...\")\n",
        "\n",
        "    # Convert the click coordinates to numpy array\n",
        "    click_coordinates = np.array(click_coordinates)\n",
        "\n",
        "    # Print the extreme coordinates\n",
        "    print(\"Extreme Coordinates:\")\n",
        "    for coordinate in click_coordinates:\n",
        "        x, y = coordinate\n",
        "        print(f\"x: {x}, y: {y}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "rpznPhxgeZll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import cv2\n",
        "import numpy as np\n",
        "from streamlit_drawable_canvas import st_canvas\n",
        "\n",
        "def main():\n",
        "    st.title(\"Click on Extreme Coordinates\")\n",
        "    st.write(\"Select the extreme points on the input image\")\n",
        "\n",
        "    # Upload image file\n",
        "    uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        # Read the uploaded image\n",
        "        file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)\n",
        "        image = cv2.imdecode(file_bytes, 1)\n",
        "\n",
        "        # Display the image\n",
        "        st.image(image, channels=\"BGR\", caption=\"Uploaded Image\")\n",
        "\n",
        "        # Create a canvas to draw on the image\n",
        "        canvas_image = st_canvas(\n",
        "            fill_color=\"rgba(255, 0, 0, 0.3)\",  # Set drawing color and opacity\n",
        "            stroke_width=2,\n",
        "            stroke_color=\"red\",\n",
        "            background_image=image,\n",
        "            update_streamlit=True,\n",
        "            height=image.shape[0],  # Set canvas height same as image\n",
        "            drawing_mode=\"freedraw\",\n",
        "            key=\"canvas\",\n",
        "        )\n",
        "\n",
        "        # Get the drawn coordinates\n",
        "        if canvas_image is not None:\n",
        "            # Convert to grayscale\n",
        "            gray_image = cv2.cvtColor(canvas_image.astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Get non-zero pixel coordinates\n",
        "            coordinates = np.argwhere(gray_image > 0)\n",
        "\n",
        "            # Display the clicked coordinates\n",
        "            st.write(\"Clicked Coordinates:\")\n",
        "            for coordinate in coordinates:\n",
        "                x, y = coordinate[1], coordinate[0]  # Swap x and y coordinates\n",
        "                st.write(f\"x: {x}, y: {y}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "7c-3eQ74cex_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from streamlit_cropper import st_cropper\n",
        "\n",
        "def main():\n",
        "    # Set up Streamlit layout\n",
        "    st.title(\"Click Extreme Points on Image\")\n",
        "    st.sidebar.header(\"Image Selection\")\n",
        "\n",
        "    # Upload image\n",
        "    uploaded_image = st.sidebar.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "    # Display uploaded image\n",
        "    if uploaded_image is not None:\n",
        "        image = Image.open(uploaded_image)\n",
        "        st.image(image)\n",
        "\n",
        "        # Get cropped image\n",
        "        cropped_image = st_cropper(image)\n",
        "\n",
        "        # Display the coordinates of the cropped area\n",
        "        st.sidebar.write(f\"Cropped Area: {cropped_image}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "gh93CsbI3o7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from streamlit_cropper import st_cropper\n",
        "\n",
        "def main():\n",
        "    st.title(\"Corner Coordinate Logger\")\n",
        "\n",
        "    # Upload image\n",
        "    uploaded_image = st.file_uploader(\"Upload Image\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
        "\n",
        "    if uploaded_image is not None:\n",
        "        # Read the uploaded image\n",
        "        image = read_image(uploaded_image)\n",
        "\n",
        "        # Display the original image\n",
        "        st.image(image, caption=\"Original Image\")\n",
        "\n",
        "        # Crop the image and get the coordinates\n",
        "        cropped_image, (top_left, bottom_right) = st_cropper(image, box_color=(255, 0, 0), box_thickness=2)\n",
        "\n",
        "        # Display the cropped image\n",
        "        st.image(cropped_image, caption=\"Cropped Image\")\n",
        "\n",
        "        # Display the coordinates\n",
        "        st.write(f\"Top-Left Corner: {top_left}\")\n",
        "        st.write(f\"Bottom-Right Corner: {bottom_right}\")\n",
        "\n",
        "def read_image(image):\n",
        "    # Read the image using the appropriate library (e.g., PIL, OpenCV, etc.)\n",
        "    # Return the image as a numpy array or PIL Image object\n",
        "    return image\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "1Ygvoq9bfP_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###########hey\n",
        "import streamlit as st\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def main():\n",
        "    st.title(\"Click on Extreme Coordinates\")\n",
        "    st.write(\"Select the extreme points on the input image\")\n",
        "\n",
        "    # Upload image file\n",
        "    uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        # Read the uploaded image\n",
        "        file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)\n",
        "        image = cv2.imdecode(file_bytes, 1)\n",
        "\n",
        "        # Display the image\n",
        "        st.image(image, channels=\"BGR\", caption=\"Uploaded Image\")\n",
        "\n",
        "        # Get extreme coordinates on image click\n",
        "        st.write(\"Click on the extreme points of the object:\")\n",
        "        clicked_points = st.image_processing_tools.drawing_canvas(\n",
        "            image,\n",
        "            tool=\"point\",\n",
        "            color=\"red\",\n",
        "            drawing_mode=\"freedraw\",\n",
        "            key=\"canvas\",\n",
        "        )\n",
        "\n",
        "        # Display the clicked coordinates\n",
        "        if clicked_points is not None:\n",
        "            st.write(\"Clicked Coordinates:\")\n",
        "            for point in clicked_points:\n",
        "                st.write(f\"x: {point['x']}, y: {point['y']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Cb4mJTAobJMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from streamlit_cropper import st_cropper\n",
        "\n",
        "def main():\n",
        "    st.title(\"Image Cropper\")\n",
        "\n",
        "    # Upload image\n",
        "    uploaded_image = st.file_uploader(\"Upload Image\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
        "\n",
        "    if uploaded_image is not None:\n",
        "        # Read the uploaded image using OpenCV\n",
        "        image = read_image(uploaded_image)\n",
        "\n",
        "        # Display the original image\n",
        "        st.image(image, caption=\"Original Image\")\n",
        "\n",
        "        # Use st_cropper to crop the image\n",
        "        cropped_image, coords = st_cropper(image, realtime_update=True, box_color=(255, 0, 0))\n",
        "\n",
        "        # Display the cropped image\n",
        "        st.image(cropped_image, caption=\"Cropped Image\")\n",
        "\n",
        "        # Get the extreme points coordinates\n",
        "        min_x, min_y = coords[0]\n",
        "        max_x, max_y = coords[1]\n",
        "\n",
        "        # Display the coordinates\n",
        "        st.write(\"Top Left Coordinate: ({}, {})\".format(min_x, min_y))\n",
        "        st.write(\"Bottom Right Coordinate: ({}, {})\".format(max_x, max_y))\n",
        "\n",
        "def read_image(image_file):\n",
        "    image_data = image_file.read()\n",
        "    image_array = np.frombuffer(image_data, np.uint8)\n",
        "    image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n",
        "    return image\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Hr2JzU8Q1gIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def main():\n",
        "    st.title(\"Image Cropper\")\n",
        "\n",
        "    # Upload image\n",
        "    uploaded_image = st.file_uploader(\"Upload Image\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
        "\n",
        "    if uploaded_image is not None:\n",
        "        # Read the uploaded image using OpenCV\n",
        "        image = read_image(uploaded_image)\n",
        "\n",
        "        # Display the original image\n",
        "        st.image(image, caption=\"Original Image\")\n",
        "\n",
        "        # Draw a rectangular selection to crop the image\n",
        "        cropped_image = st_cropper(image)\n",
        "\n",
        "        # Display the cropped image\n",
        "        st.image(cropped_image, caption=\"Cropped Image\")\n",
        "\n",
        "def read_image(image_file):\n",
        "    image_data = np.fromstring(image_file.read(), np.uint8)\n",
        "    image = cv2.imdecode(image_data, cv2.IMREAD_UNCHANGED)\n",
        "    return image\n",
        "\n",
        "def st_cropper(image):\n",
        "    image_copy = image.copy()\n",
        "    height, width = image_copy.shape[:2]\n",
        "\n",
        "    # Create a blank mask image\n",
        "    mask = np.zeros_like(image_copy)\n",
        "\n",
        "    # Draw a rectangular selection using mouse events\n",
        "    rect_start = None\n",
        "    rect_end = None\n",
        "    rect_color = (0, 255, 0)  # Green color\n",
        "\n",
        "    def mouse_callback(event, x, y, flags, param):\n",
        "        nonlocal rect_start, rect_end\n",
        "\n",
        "        if event == cv2.EVENT_LBUTTONDOWN:\n",
        "            rect_start = (x, y)\n",
        "\n",
        "        elif event == cv2.EVENT_LBUTTONUP:\n",
        "            rect_end = (x, y)\n",
        "            cv2.rectangle(mask, rect_start, rect_end, (255, 255, 255), -1)\n",
        "\n",
        "    cv2.namedWindow(\"Cropping\")\n",
        "    cv2.setMouseCallback(\"Cropping\", mouse_callback)\n",
        "\n",
        "    # Keep updating the display until cropping is done\n",
        "    while True:\n",
        "        # Apply the mask to the image to show the rectangular selection\n",
        "        image_with_mask = cv2.addWeighted(image_copy, 0.7, mask, 0.3, 0)\n",
        "\n",
        "        # Display the image with mask\n",
        "        cv2.imshow(\"Cropping\", image_with_mask)\n",
        "\n",
        "        # Press 'Esc' key to finish cropping\n",
        "        key = cv2.waitKey(1) & 0xFF\n",
        "        if key == 27:\n",
        "            break\n",
        "\n",
        "    # Close the cropping window\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    # Apply the mask to the original image to get the cropped image\n",
        "    cropped_image = cv2.bitwise_and(image_copy, mask)\n",
        "\n",
        "    return cropped_image\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "LVP1S4Q5LRM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wiTuj89bZA8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "def main():\n",
        "    st.title(\"Image Cropper\")\n",
        "\n",
        "    # Upload image file\n",
        "    uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        # Load image\n",
        "        image = Image.open(uploaded_file)\n",
        "\n",
        "        # Display image\n",
        "        st.image(image, caption=\"Original Image\")\n",
        "\n",
        "        # Get user input points\n",
        "        cropped_image = get_cropped_image(image)\n",
        "\n",
        "        # Display cropped image\n",
        "        st.image(cropped_image, caption=\"Cropped Image\")\n",
        "\n",
        "def get_cropped_image(image):\n",
        "    # Create copy of the original image\n",
        "    image_copy = image.copy()\n",
        "\n",
        "    # Create image draw object\n",
        "    draw = ImageDraw.Draw(image_copy)\n",
        "\n",
        "    # Display instructions to user\n",
        "    st.write(\"Instructions:\")\n",
        "    st.write(\"1. Click on the top-left corner of the desired crop region.\")\n",
        "    st.write(\"2. Click on the bottom-right corner of the desired crop region.\")\n",
        "    st.write(\"3. The cropped image will be displayed below.\")\n",
        "\n",
        "    # Get user input points\n",
        "    with st.beta_expander(\"Image with Crop Points\"):\n",
        "        # Display image for user input\n",
        "        st.image(image_copy, use_column_width=True, caption=\"Select the crop points\")\n",
        "\n",
        "        # Get crop points\n",
        "        crop_points = st.session_state.get(\"crop_points\", [])\n",
        "\n",
        "        if len(crop_points) < 2:\n",
        "            # Wait for user clicks\n",
        "            event_result = st.pydeck_chart(screenshot=\"True\", use_container_width=True)\n",
        "            if event_result:\n",
        "                x, y = event_result[\"layers\"][0][\"data\"][\"lat\"][0], event_result[\"layers\"][0][\"data\"][\"lon\"][0]\n",
        "                crop_points.append((x, y))\n",
        "                st.session_state[\"crop_points\"] = crop_points\n",
        "\n",
        "        # Draw rectangle on the image\n",
        "        if len(crop_points) == 2:\n",
        "            draw.rectangle([crop_points[0], crop_points[1]], outline=\"red\")\n",
        "\n",
        "    # Crop the image\n",
        "    if len(crop_points) == 2:\n",
        "        cropped_image = image.crop([crop_points[0][0], crop_points[0][1], crop_points[1][0], crop_points[1][1]])\n",
        "        return cropped_image\n",
        "\n",
        "    return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "OI39Sz-fZDib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from PIL import Image, ImageDraw\n",
        "import pandas as pd\n",
        "\n",
        "def main():\n",
        "    st.title(\"Image Cropper\")\n",
        "\n",
        "    # Upload image file\n",
        "    uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        # Load image\n",
        "        image = Image.open(uploaded_file)\n",
        "\n",
        "        # Display image\n",
        "        st.image(image, caption=\"Original Image\")\n",
        "\n",
        "        # Get user input points\n",
        "        cropped_image = get_cropped_image(image)\n",
        "\n",
        "        # Display cropped image\n",
        "        st.image(cropped_image, caption=\"Cropped Image\")\n",
        "\n",
        "def get_cropped_image(image):\n",
        "    # Create copy of the original image\n",
        "    image_copy = image.copy()\n",
        "\n",
        "    # Create image draw object\n",
        "    draw = ImageDraw.Draw(image_copy)\n",
        "\n",
        "    # Display instructions to user\n",
        "    st.write(\"Instructions:\")\n",
        "    st.write(\"1. Click on the top-left corner of the desired crop region.\")\n",
        "    st.write(\"2. Click on the bottom-right corner of the desired crop region.\")\n",
        "    st.write(\"3. The cropped image will be displayed below.\")\n",
        "\n",
        "    # Get user input points\n",
        "    crop_points = st.session_state.get(\"crop_points\", [])\n",
        "\n",
        "    if len(crop_points) < 2:\n",
        "        # Wait for user clicks\n",
        "        clicked_positions = st.image(image_copy, use_column_width=True, caption=\"Select the crop points\", format=\"PNG\")\n",
        "\n",
        "        if clicked_positions is not None:\n",
        "            # Store the clicked positions\n",
        "            if \"coordinates\" in clicked_positions:\n",
        "                crop_points.append(clicked_positions[\"coordinates\"])\n",
        "                st.session_state[\"crop_points\"] = crop_points\n",
        "\n",
        "    # Draw rectangle on the image\n",
        "    if len(crop_points) == 2:\n",
        "        draw.rectangle([crop_points[0], crop_points[1]], outline=\"red\")\n",
        "\n",
        "    # Crop the image\n",
        "    if len(crop_points) == 2:\n",
        "        cropped_image = image.crop((crop_points[0][0], crop_points[0][1], crop_points[1][0], crop_points[1][1]))\n",
        "        return cropped_image\n",
        "\n",
        "    return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "2oTIuWOUZ8Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import streamlit as st\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "def main():\n",
        "    st.title(\"Image Cropper\")\n",
        "\n",
        "    # Upload image file\n",
        "    uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        # Load image\n",
        "        image = Image.open(uploaded_file)\n",
        "\n",
        "        # Display image\n",
        "        st.image(image, caption=\"Original Image\")\n",
        "\n",
        "        # Get user input points\n",
        "        cropped_image = get_cropped_image(image)\n",
        "\n",
        "        # Display cropped image\n",
        "        st.image(cropped_image, caption=\"Cropped Image\")\n",
        "\n",
        "def get_cropped_image(image):\n",
        "    # Create copy of the original image\n",
        "    image_copy = image.copy()\n",
        "\n",
        "    # Create image draw object\n",
        "    draw = ImageDraw.Draw(image_copy)\n",
        "\n",
        "    # Display instructions to user\n",
        "    st.write(\"Instructions:\")\n",
        "    st.write(\"1. Click on the top-left corner of the desired crop region.\")\n",
        "    st.write(\"2. Click on the bottom-right corner of the desired crop region.\")\n",
        "    st.write(\"3. The cropped image will be displayed below.\")\n",
        "\n",
        "    # Get user input points\n",
        "    crop_points = st.session_state.get(\"crop_points\", [])\n",
        "\n",
        "    if len(crop_points) < 2:\n",
        "        # Wait for user clicks\n",
        "        clicked_positions = st.image(image_copy, use_column_width=True, caption=\"Select the crop points\", format=\"PNG\")\n",
        "\n",
        "        if st.button(\"Click to capture point\"):\n",
        "            # Store the clicked positions\n",
        "            crop_points.append(st.session_state.cursor_xy)\n",
        "            st.session_state[\"crop_points\"] = crop_points\n",
        "\n",
        "    # Draw rectangle on the image\n",
        "    if len(crop_points) == 2:\n",
        "        draw.rectangle([crop_points[0], crop_points[1]], outline=\"red\")\n",
        "\n",
        "    # Crop the image\n",
        "    if len(crop_points) == 2:\n",
        "        cropped_image = image.crop((crop_points[0][0], crop_points[0][1], crop_points[1][0], crop_points[1][1]))\n",
        "        return cropped_image\n",
        "\n",
        "    return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "9MgXaXU4kor1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from PIL import Image\n",
        "\n",
        "def main():\n",
        "    st.title(\"Corner Coordinate Logger\")\n",
        "\n",
        "    # Upload image\n",
        "    uploaded_image = st.file_uploader(\"Upload Image\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
        "\n",
        "    if uploaded_image is not None:\n",
        "        # Convert uploaded image to PIL Image object\n",
        "        pil_image = Image.open(uploaded_image)\n",
        "\n",
        "        # Display the uploaded image\n",
        "        st.image(pil_image, use_column_width=True)\n",
        "\n",
        "        # Get the coordinates of the corners\n",
        "        top_left, bottom_right = st_cropper(pil_image)\n",
        "\n",
        "        # Display the coordinates\n",
        "        st.write(f\"Top-Left Corner: {top_left}\")\n",
        "        st.write(f\"Bottom-Right Corner: {bottom_right}\")\n",
        "\n",
        "def st_cropper(image):\n",
        "    # Add code for cropper component here\n",
        "    # Implement the logic to get the coordinates of the corners\n",
        "    # Return the coordinates as top_left and bottom_right\n",
        "    top_left = (0, 0)  # Placeholder for top-left corner\n",
        "    bottom_right = (image.width, image.height)  # Placeholder for bottom-right corner\n",
        "    return top_left, bottom_right\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "gBQBVITwecZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the image\n",
        "image = cv2.imread('image.jpg')\n",
        "\n",
        "# Convert the image to grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Apply Gaussian blur to reduce noise\n",
        "blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "# Perform text detection\n",
        "text_detector = cv2.text.TextDetectorCNN_create(\"path/to/opencv_text_detection.pb\")\n",
        "_, text_regions = text_detector.detect(blur)\n",
        "\n",
        "# Create a mask for the text regions\n",
        "text_mask = np.zeros_like(gray)\n",
        "for region in text_regions:\n",
        "    x, y, w, h = region[0]\n",
        "    cv2.rectangle(text_mask, (x, y), (x + w, y + h), (255), cv2.FILLED)\n",
        "\n",
        "# Find contours excluding text regions\n",
        "contours, _ = cv2.findContours(cv2.bitwise_not(text_mask), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Draw contours on the original image\n",
        "contour_image = cv2.drawContours(image.copy(), contours, contourIdx=-1, color=(0, 255, 0), thickness=2)\n",
        "\n",
        "# Display the result\n",
        "cv2.imshow('Text and Contour Detection', np.hstack((image, contour_image)))\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "qUjgrC6KVLqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import cv2\n",
        "\n",
        "def main():\n",
        "    # Set up Streamlit layout\n",
        "    st.title(\"Click Extreme Points on Image\")\n",
        "    st.sidebar.header(\"Image Selection\")\n",
        "\n",
        "    # Upload image\n",
        "    uploaded_image = st.sidebar.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "    # Display uploaded image\n",
        "    if uploaded_image is not None:\n",
        "        image = cv2.imdecode(np.fromstring(uploaded_image.read(), np.uint8), 1)\n",
        "        st.image(image, channels=\"BGR\")\n",
        "\n",
        "        # Get image shape\n",
        "        height, width, _ = image.shape\n",
        "\n",
        "        # Create a canvas to display clicked points\n",
        "        canvas = st.image(np.zeros_like(image, dtype=np.uint8), width=width, channels=\"BGR\", use_column_width=True)\n",
        "\n",
        "        # Initialize list to store clicked points\n",
        "        clicked_points = []\n",
        "\n",
        "        # Function to handle mouse clicks on the image\n",
        "        def handle_click_event(event, x, y, flags, param):\n",
        "            if event == cv2.EVENT_LBUTTONDOWN:\n",
        "                # Add clicked point to the list\n",
        "                clicked_points.append((x, y))\n",
        "\n",
        "                # Draw a circle at the clicked point on the canvas\n",
        "                cv2.circle(canvas.image, (x, y), 5, (0, 0, 255), -1)\n",
        "\n",
        "                # Update the displayed image\n",
        "                canvas.image = cv2.cvtColor(canvas.image, cv2.COLOR_BGR2RGB)\n",
        "                canvas.image_widget.value = canvas.image\n",
        "\n",
        "                # Display the coordinates of the clicked point\n",
        "                st.sidebar.write(f\"Clicked Point: ({x}, {y})\")\n",
        "\n",
        "        # Set up mouse click event listener\n",
        "        cv2.namedWindow(\"Image\")\n",
        "        cv2.setMouseCallback(\"Image\", handle_click_event)\n",
        "\n",
        "        # Wait for user to click on the image\n",
        "        while len(clicked_points) < 2:\n",
        "            cv2.imshow(\"Image\", image)\n",
        "            cv2.waitKey(1)\n",
        "\n",
        "        # Close the OpenCV windows\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "    else:\n",
        "        st.write(\"Please upload an image on the sidebar\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "J0E2b8m_08QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from streamlit_canvas import st_canvas\n",
        "import numpy as np\n",
        "\n",
        "def main():\n",
        "    # Set up Streamlit layout\n",
        "    st.title(\"Click Extreme Points on Image\")\n",
        "    st.sidebar.header(\"Image Selection\")\n",
        "\n",
        "    # Upload image\n",
        "    uploaded_image = st.sidebar.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "    # Display uploaded image\n",
        "    if uploaded_image is not None:\n",
        "        image = np.array(Image.open(uploaded_image))\n",
        "        st.image(image)\n",
        "\n",
        "        # Get image shape\n",
        "        height, width, _ = image.shape\n",
        "\n",
        "        # Create a canvas to display clicked points\n",
        "        canvas_result = st_canvas(\n",
        "            fill_color=\"rgba(255, 0, 0, 0.3)\",  # Red color with opacity\n",
        "            stroke_width=2,\n",
        "            stroke_color=\"rgba(255, 0, 0, 1)\",  # Red color\n",
        "            background_color=\"rgba(0, 0, 0, 0)\",  # Transparent background\n",
        "            height=height,\n",
        "            width=width,\n",
        "            drawing_mode=\"freedraw\",\n",
        "            key=\"canvas\",\n",
        "        )\n",
        "\n",
        "        # Initialize list to store clicked points\n",
        "        clicked_points = []\n",
        "\n",
        "        # Function to handle button click event\n",
        "        def handle_button_click():\n",
        "            st.sidebar.write(f\"Clicked Points: {clicked_points}\")\n",
        "\n",
        "        # Wait for user to click on the image\n",
        "        while len(clicked_points) < 2:\n",
        "            # Keep the app running without blocking\n",
        "            clicked_points = canvas_result.json_data[\"objects\"][0][\"data\"]\n",
        "            st.sidebar.write(\"Waiting for points...\")\n",
        "            st.sidebar.write(f\"Clicked Points: {clicked_points}\")\n",
        "            st.empty()\n",
        "\n",
        "        # Display the coordinates of the clicked points\n",
        "        st.sidebar.write(f\"Clicked Points: {clicked_points}\")\n",
        "        st.sidebar.button(\"Submit\", on_click=handle_button_click)\n",
        "\n",
        "    else:\n",
        "        st.write(\"Please upload an image on the sidebar\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "EPyZzTUO2n6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Load the image\n",
        "image = cv2.imread('image.jpg')\n",
        "\n",
        "# Convert the image to grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Apply Gaussian blur to reduce noise\n",
        "blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "# Perform text detection\n",
        "text_detection = cv2.text.detectText(blur, cv2.text.OCR_KNN_MODEL)\n",
        "\n",
        "# Get the detected text regions\n",
        "text_regions = text_detection[0]\n",
        "\n",
        "# Create a mask for the text regions\n",
        "text_mask = cv2.drawContours(\n",
        "    np.zeros_like(gray), text_regions, contourIdx=-1, color=255, thickness=cv2.FILLED\n",
        ")\n",
        "\n",
        "# Find contours excluding text regions\n",
        "contours, _ = cv2.findContours(cv2.bitwise_not(text_mask), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Draw contours on the original image\n",
        "contour_image = cv2.drawContours(image.copy(), contours, contourIdx=-1, color=(0, 255, 0), thickness=2)\n",
        "\n",
        "# Display the result\n",
        "cv2.imshow('Text and Contour Detection', np.hstack((image, contour_image)))\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "06PNtj1HUN9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "def main():\n",
        "    st.title(\"Image Coordinates App\")\n",
        "\n",
        "    # Upload the image\n",
        "    uploaded_file = st.file_uploader(\"Upload Image\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        # Display the image with coordinates\n",
        "        coords = st.image_coordinates(uploaded_file, key=\"image\")\n",
        "\n",
        "        if st.button(\"Save Point\") and coords is not None:\n",
        "            coords_list = st.session_state.get(\"coords_list\", [])\n",
        "            coords_list.append(coords)\n",
        "            st.session_state[\"coords_list\"] = coords_list\n",
        "\n",
        "        if coords is not None:\n",
        "            st.write(\"Clicked Coordinates:\", coords)\n",
        "\n",
        "        coords_list = st.session_state.get(\"coords_list\", [])\n",
        "        if coords_list:\n",
        "            st.write(\"All Clicked Coordinates:\")\n",
        "            for i, coord in enumerate(coords_list):\n",
        "                st.write(f\"Point {i+1}: {coord}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "q0_dJVB1xwKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py (API)\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the trained ML model\n",
        "model = joblib.load('your_model.pkl')\n",
        "\n",
        "def process_image(image):\n",
        "    # Convert the image bytes to a numpy array\n",
        "    nparr = np.frombuffer(image.read(), np.uint8)\n",
        "\n",
        "    # Decode the numpy array to an image\n",
        "    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "\n",
        "    # Preprocess the image (e.g., resize, normalize, etc.)\n",
        "    # ...\n",
        "\n",
        "    return img\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    # Get the uploaded images from the request\n",
        "    image1 = request.files['image1']\n",
        "    image2 = request.files['image2']\n",
        "\n",
        "    # Process the images\n",
        "    processed_image1 = process_image(image1)\n",
        "    processed_image2 = process_image(image2)\n",
        "\n",
        "    # Perform prediction using the loaded model and the processed images\n",
        "    prediction = model.predict([processed_image1, processed_image2])\n",
        "\n",
        "    # Create a dataframe of the predicted values\n",
        "    df = pd.DataFrame(prediction, columns=['Predicted Values'])\n",
        "\n",
        "    # Return the dataframe as JSON response\n",
        "    return df.to_json()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()\n",
        "\n",
        "# streamlit_app.py (Streamlit)\n",
        "import streamlit as st\n",
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "# Define the API URL\n",
        "API_URL = 'http://localhost:5000/predict'\n",
        "\n",
        "# Streamlit UI\n",
        "st.title('ML Model API Demo')\n",
        "\n",
        "# Get user input\n",
        "uploaded_image1 = st.file_uploader('Upload Image 1', type=['jpg', 'jpeg', 'png'])\n",
        "uploaded_image2 = st.file_uploader('Upload Image 2', type=['jpg', 'jpeg', 'png'])\n",
        "\n",
        "# Process user input\n",
        "if uploaded_image1 and uploaded_image2:\n",
        "    # Prepare the image files as bytes\n",
        "    image_bytes1 = uploaded_image1.read()\n",
        "    image_bytes2 = uploaded_image2.read()\n",
        "\n",
        "    # Send the image bytes to the API\n",
        "    files = {'image1': ('image1.jpg', io.BytesIO(image_bytes1), 'image/jpeg'),\n",
        "             'image2': ('image2.jpg', io.BytesIO(image_bytes2), 'image/jpeg')}\n",
        "    response = requests.post(API_URL, files=files)\n",
        "\n",
        "    # Convert the JSON response to a dataframe\n",
        "    df = pd.read_json(response.content)\n",
        "\n",
        "    # Display the dataframe\n",
        "    st.write('Predicted Values:')\n",
        "    st.dataframe(df)\n"
      ],
      "metadata": {
        "id": "j1PaNi1IPaLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def select_roi(image):\n",
        "    st.image(image, use_column_width=True)\n",
        "\n",
        "    st.header(\"Select Region of Interest (ROI)\")\n",
        "    st.write(\"Please follow the instructions below to select the extreme coordinates of the ROI:\")\n",
        "\n",
        "    st.markdown(\"1. Click on the **top-left** corner of the ROI.\")\n",
        "    st.markdown(\"2. Click on the **bottom-right** corner of the ROI.\")\n",
        "\n",
        "    # Create a copy of the original image to draw the rectangle\n",
        "    img_copy = image.copy()\n",
        "\n",
        "    # Create a streamlit canvas to handle mouse clicks\n",
        "    canvas = st.image(img_copy, use_column_width=True, clamp=True, channels=\"BGR\")\n",
        "\n",
        "    # Initialize variables to store the selected coordinates\n",
        "    top_left = None\n",
        "    bottom_right = None\n",
        "\n",
        "    def mouse_callback(event, x, y, flags, param):\n",
        "        nonlocal top_left, bottom_right\n",
        "\n",
        "        if event == cv2.EVENT_LBUTTONDOWN:\n",
        "            if top_left is None:\n",
        "                top_left = (x, y)\n",
        "            else:\n",
        "                bottom_right = (x, y)\n",
        "\n",
        "                # Draw a rectangle on the image copy to show the selected ROI\n",
        "                cv2.rectangle(img_copy, top_left, bottom_right, (0, 255, 0), 2)\n",
        "\n",
        "                # Update the streamlit canvas with the image copy\n",
        "                canvas.image(img_copy, channels=\"BGR\")\n",
        "\n",
        "    # Set up the mouse callback function\n",
        "    cv2.namedWindow(\"Select ROI\")\n",
        "    cv2.setMouseCallback(\"Select ROI\", mouse_callback)\n",
        "\n",
        "    # Keep the Streamlit app running until the user selects the ROI\n",
        "    while bottom_right is None:\n",
        "        st.experimental_rerun()\n",
        "\n",
        "    # Close the OpenCV window\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    return top_left, bottom_right\n",
        "\n",
        "\n",
        "# Main Streamlit app\n",
        "def main():\n",
        "    st.title(\"Image ROI Selection\")\n",
        "\n",
        "    st.header(\"Upload an Image\")\n",
        "    st.write(\"Please upload an image file (PNG, JPG, JPEG) to select the region of interest (ROI).\")\n",
        "\n",
        "    # File upload\n",
        "    uploaded_file = st.file_uploader(\"Choose an image\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        # Read the uploaded image\n",
        "        image = np.array(Image.open(uploaded_file))\n",
        "\n",
        "        # Display the original image\n",
        "        st.subheader(\"Original Image\")\n",
        "        st.image(image, use_column_width=True, channels=\"BGR\")\n",
        "\n",
        "        # Select the region of interest\n",
        "        st.header(\"Select ROI\")\n",
        "        st.write(\"Please follow the instructions to select the extreme coordinates of the ROI.\")\n",
        "\n",
        "        top_left, bottom_right = select_roi(image)\n",
        "\n",
        "        # Display the selected coordinates\n",
        "        st.header(\"Selected Coordinates\")\n",
        "        st.write(\"Top Left:\", top_left)\n",
        "        st.write(\"Bottom Right:\", bottom_right)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "-3CeqpYxt2sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ppU3gcrdt3x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "def main():\n",
        "    st.title(\"Corner Coordinate Logger\")\n",
        "\n",
        "    # Upload image\n",
        "    uploaded_image = st.file_uploader(\"Upload Image\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
        "\n",
        "    if uploaded_image is not None:\n",
        "        # Display the uploaded image\n",
        "        st.image(uploaded_image, use_column_width=True)\n",
        "\n",
        "        # Get the corner coordinates\n",
        "        coordinates = []\n",
        "        click_count = 0\n",
        "\n",
        "        # Mouse click event handler\n",
        "        def get_click_coordinates(event):\n",
        "            nonlocal click_count\n",
        "            click_count += 1\n",
        "            coordinates.append((event.x, event.y))\n",
        "\n",
        "            if click_count >= 2:\n",
        "                # Display the coordinates\n",
        "                st.write(f\"Top-Left Corner: {coordinates[0]}\")\n",
        "                st.write(f\"Bottom-Right Corner: {coordinates[1]}\")\n",
        "\n",
        "        # Register the callback for mouse click events\n",
        "        st_canvas = st.image([0], use_column_width=True, width=uploaded_image.width, clamp=True)\n",
        "        st_canvas._image_data = uploaded_image\n",
        "        st_canvas._add_bokeh_mouse_callbacks(\"mousemove\", get_click_coordinates)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "lhOx17u5YLJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from PIL import Image\n",
        "\n",
        "def main():\n",
        "    st.title(\"Corner Coordinate Logger\")\n",
        "\n",
        "    # Upload image\n",
        "    uploaded_image = st.file_uploader(\"Upload Image\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
        "\n",
        "    if uploaded_image is not None:\n",
        "        # Display the uploaded image\n",
        "        pil_image = Image.open(uploaded_image)\n",
        "        st.image(pil_image, use_column_width=True)\n",
        "\n",
        "        # Get the corner coordinates\n",
        "        coordinates = []\n",
        "        click_count = 0\n",
        "\n",
        "        # Mouse click event handler\n",
        "        def get_click_coordinates(event):\n",
        "            nonlocal click_count\n",
        "            click_count += 1\n",
        "            coordinates.append((event.x, event.y))\n",
        "\n",
        "            if click_count >= 2:\n",
        "                # Display the coordinates\n",
        "                st.write(f\"Top-Left Corner: {coordinates[0]}\")\n",
        "                st.write(f\"Bottom-Right Corner: {coordinates[1]}\")\n",
        "\n",
        "        # Register the callback for mouse click events\n",
        "        st_canvas = st.image([0], use_column_width=True, width=pil_image.width, clamp=True)\n",
        "        st\n",
        "        _canvas._image_data = pil_image\n",
        "        st_canvas._add_bokeh_mouse_callbacks(\"mousemove\", get_click_coordinates)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "q_kn1dXwY_ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from streamlit_cropper import st_cropper\n",
        "from PIL import Image\n",
        "\n",
        "def main():\n",
        "    st.title(\"Corner Coordinate Logger\")\n",
        "\n",
        "    # Upload image\n",
        "    uploaded_image = st.file_uploader(\"Upload Image\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
        "\n",
        "    if uploaded_image is not None:\n",
        "        # Display the uploaded image\n",
        "        pil_image = Image.open(uploaded_image)\n",
        "        st.image(pil_image, use_column_width=True)\n",
        "\n",
        "        # Cropper component to select corners\n",
        "        cropped_image = st_cropper(pil_image)\n",
        "\n",
        "        if cropped_image is not None:\n",
        "            # Get the coordinates of the cropped image\n",
        "            top_left = (cropped_image[0][0], cropped_image[0][1])\n",
        "            bottom_right = (cropped_image[1][0], cropped_image[1][1])\n",
        "\n",
        "            # Display the coordinates\n",
        "            st.write(f\"Top-Left Corner: {top_left}\")\n",
        "            st.write(f\"Bottom-Right Corner: {bottom_right}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "t0LoL-Spbmd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py (API)\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import base64\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the trained ML model\n",
        "model = joblib.load('your_model.pkl')\n",
        "\n",
        "def process_image(image_data):\n",
        "    # Decode the base64 encoded image data\n",
        "    nparr = np.frombuffer(base64.b64decode(image_data), np.uint8)\n",
        "\n",
        "    # Decode the numpy array to an image\n",
        "    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "\n",
        "    # Preprocess the image (e.g., resize, normalize, etc.)\n",
        "    # ...\n",
        "\n",
        "    return img\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    # Get the uploaded images from the request\n",
        "    image1_data = request.form['image1']\n",
        "    image2_data = request.form['image2']\n",
        "\n",
        "    # Process the images\n",
        "    processed_image1 = process_image(image1_data)\n",
        "    processed_image2 = process_image(image2_data)\n",
        "\n",
        "    # Perform prediction using the loaded model and the processed images\n",
        "    prediction = model.predict([processed_image1, processed_image2])\n",
        "\n",
        "    # Create a dataframe of the predicted values\n",
        "    df = pd.DataFrame(prediction, columns=['Predicted Values'])\n",
        "\n",
        "    # Return the dataframe as JSON response\n",
        "    return df.to_json()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()\n",
        "\n",
        "# streamlit_app.py (Streamlit)\n",
        "import streamlit as st\n",
        "import requests\n",
        "import pandas as pd\n",
        "import base64\n",
        "import io\n",
        "\n",
        "# Define the API URL\n",
        "API_URL = 'http://localhost:5000/predict'\n",
        "\n",
        "# Streamlit UI\n",
        "st.title('ML Model API Demo')\n",
        "\n",
        "# Get user input\n",
        "uploaded_image1 = st.file_uploader('Upload Image 1', type=['jpg', 'jpeg', 'png'])\n",
        "uploaded_image2 = st.file_uploader('Upload Image 2', type=['jpg', 'jpeg', 'png'])\n",
        "\n",
        "# Process user input\n",
        "if uploaded_image1 and uploaded_image2:\n",
        "    # Save the uploaded images\n",
        "    image_path1 = 'uploaded_image1.jpg'\n",
        "    image_path2 = 'uploaded_image2.jpg'\n",
        "    with open(image_path1, 'wb') as f1, open(image_path2, 'wb') as f2:\n",
        "        f1.write(uploaded_image1.getvalue())\n",
        "        f2.write(uploaded_image2.getvalue())\n",
        "\n",
        "    # Read the saved images as bytes\n",
        "    with open(image_path1, 'rb') as f1, open(image_path2, 'rb') as f2:\n",
        "        image_bytes1 = f1.read()\n",
        "        image_bytes2 = f2.read()\n",
        "\n",
        "    # Convert the image bytes to base64 encoded strings\n",
        "    image_data1 = base64.b64encode(image_bytes1).decode('utf-8')\n",
        "    image_data2 = base64.b64encode(image_bytes2).decode('utf-8')\n",
        "\n",
        "    # Send the base64 encoded image data to the API\n",
        "    data = {'image1': image_data1, 'image2': image_data2}\n",
        "    response = requests.post(API_URL, data=data)\n",
        "\n",
        "    # Convert the JSON response to a dataframe\n",
        "    df = pd.read_json(response.content)\n",
        "\n",
        "    # Display the dataframe\n",
        "    st.write('Predicted Values:')\n",
        "    st.dataframe(df)\n"
      ],
      "metadata": {
        "id": "HppPm8Zy1nGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py (API)\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import base64\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the trained ML model\n",
        "model = joblib.load('your_model.pkl')\n",
        "\n",
        "def process_image(image_data):\n",
        "    # Decode the base64 encoded image data\n",
        "    nparr = np.frombuffer(base64.b64decode(image_data), np.uint8)\n",
        "\n",
        "    # Decode the numpy array to an image\n",
        "    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "\n",
        "    # Preprocess the image (e.g., resize, normalize, etc.)\n",
        "    # ...\n",
        "\n",
        "    return img\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    # Get the uploaded images from the request\n",
        "    image1_data = request.form['image1']\n",
        "    image2_data = request.form['image2']\n",
        "\n",
        "    # Process the images\n",
        "    processed_image1 = process_image(image1_data)\n",
        "    processed_image2 = process_image(image2_data)\n",
        "\n",
        "    # Perform prediction using the loaded model and the processed images\n",
        "    prediction = model.predict([processed_image1, processed_image2])\n",
        "\n",
        "    # Create a dataframe of the predicted values\n",
        "    df = pd.DataFrame(prediction, columns=['Predicted Values'])\n",
        "\n",
        "    # Return the dataframe as JSON response\n",
        "    return df.to_json()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()\n",
        "\n",
        "# streamlit_app.py (Streamlit)\n",
        "import streamlit as st\n",
        "import requests\n",
        "import pandas as pd\n",
        "import base64\n",
        "\n",
        "# Define the API URL\n",
        "API_URL = 'http://localhost:5000/predict'\n",
        "\n",
        "# Streamlit UI\n",
        "st.title('ML Model API Demo')\n",
        "\n",
        "# Get user input\n",
        "uploaded_image1 = st.file_uploader('Upload Image 1', type=['jpg', 'jpeg', 'png'])\n",
        "uploaded_image2 = st.file_uploader('Upload Image 2', type=['jpg', 'jpeg', 'png'])\n",
        "\n",
        "# Process user input\n",
        "if uploaded_image1 and uploaded_image2:\n",
        "    # Read the image files as bytes\n",
        "    image_bytes1 = uploaded_image1.read()\n",
        "    image_bytes2 = uploaded_image2.read()\n",
        "\n",
        "    # Convert the image bytes to base64 encoded strings\n",
        "    image_data1 = base64.b64encode(image_bytes1).decode('utf-8')\n",
        "    image_data2 = base64.b64encode(image_bytes2).decode('utf-8')\n",
        "\n",
        "    # Send the base64 encoded image data to the API\n",
        "    data = {'image1': image_data1, 'image2': image_data2}\n",
        "    response = requests.post(API_URL, data=data)\n",
        "\n",
        "    # Convert the JSON response to a dataframe\n",
        "    df = pd.read_json(response.content)\n",
        "\n",
        "    # Display the dataframe\n",
        "    st.write('Predicted Values:')\n",
        "    st.dataframe(df)\n"
      ],
      "metadata": {
        "id": "XYIKqYUvAeca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Gm6GmPi61n4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py (API)\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the trained ML model\n",
        "model = joblib.load('your_model.pkl')\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    # Get the uploaded images from the request\n",
        "    image1 = request.files['image1']\n",
        "    image2 = request.files['image2']\n",
        "\n",
        "    # Save the uploaded images\n",
        "    image1_path = 'uploaded_image1.jpg'\n",
        "    image2_path = 'uploaded_image2.jpg'\n",
        "    image1.save(image1_path)\n",
        "    image2.save(image2_path)\n",
        "\n",
        "    # Perform prediction using the loaded model and the image paths\n",
        "    prediction = model.predict([image1_path, image2_path])\n",
        "\n",
        "    # Prepare the response\n",
        "    response = {\n",
        "        'prediction': prediction.tolist()\n",
        "    }\n",
        "\n",
        "    return jsonify(response)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()\n",
        "\n",
        "# streamlit_app.py (Streamlit)\n",
        "import streamlit as st\n",
        "import requests\n",
        "\n",
        "# Define the API URL\n",
        "API_URL = 'http://localhost:5000/predict'\n",
        "\n",
        "# Streamlit UI\n",
        "st.title('ML Model API Demo')\n",
        "\n",
        "# Get user input\n",
        "uploaded_image1 = st.file_uploader('Upload Image 1', type=['jpg', 'jpeg', 'png'])\n",
        "uploaded_image2 = st.file_uploader('Upload Image 2', type=['jpg', 'jpeg', 'png'])\n",
        "\n",
        "# Process user input\n",
        "if uploaded_image1 and uploaded_image2:\n",
        "    # Send the uploaded images to the API\n",
        "    files = {'image1': uploaded_image1, 'image2': uploaded_image2}\n",
        "    response = requests.post(API_URL, files=files)\n",
        "\n",
        "    # Extract the prediction from the response\n",
        "    prediction = response.json()['prediction']\n",
        "\n",
        "    # Display the prediction\n",
        "    st.write('Prediction:', prediction)\n"
      ],
      "metadata": {
        "id": "WkNkDTFf8U4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MRW-Code/cmac_particle_flow.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irftjGuQ6odc",
        "outputId": "097102f2-7e58-42e5-85d0-9050b726084e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'cmac_particle_flow' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cohesive='/content/cmac_particle_flow/images/Cohesive'\n",
        "easyflow='/content/cmac_particle_flow/images/Easyflowing'\n",
        "freeflow='/content/cmac_particle_flow/images/Freeflowing'"
      ],
      "metadata": {
        "id": "07RCuClT6xjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths=[]\n",
        "labels=[]\n",
        "import os\n",
        "\n",
        "for i in os.listdir(cohesive):\n",
        "  image_paths.append(os.path.join(cohesive,i))\n",
        "  labels.append(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "0JUbfmgK64LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in os.listdir(easyflow):\n",
        "  image_paths.append(os.path.join(easyflow,i))\n",
        "  labels.append(1)"
      ],
      "metadata": {
        "id": "h5VoNi7_64V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in os.listdir(freeflow):\n",
        "  image_paths.append(os.path.join(freeflow,i))\n",
        "  labels.append(2)"
      ],
      "metadata": {
        "id": "3JCDVL_r64Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "image_data = {'path': image_paths, 'label': labels}\n",
        "df = pd.DataFrame(data=image_data)"
      ],
      "metadata": {
        "id": "kyZEJv1W64cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_jJj5HC64fs",
        "outputId": "06beda66-fe29-400c-ad8c-0121429abdf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 path  label\n",
            "0   /content/cmac_particle_flow/images/Cohesive/Az...      0\n",
            "1   /content/cmac_particle_flow/images/Cohesive/Pa...      0\n",
            "2   /content/cmac_particle_flow/images/Cohesive/1-...      0\n",
            "3   /content/cmac_particle_flow/images/Cohesive/Mo...      0\n",
            "4   /content/cmac_particle_flow/images/Cohesive/Li...      0\n",
            "..                                                ...    ...\n",
            "92  /content/cmac_particle_flow/images/Freeflowing...      2\n",
            "93  /content/cmac_particle_flow/images/Freeflowing...      2\n",
            "94  /content/cmac_particle_flow/images/Freeflowing...      2\n",
            "95  /content/cmac_particle_flow/images/Freeflowing...      2\n",
            "96  /content/cmac_particle_flow/images/Freeflowing...      2\n",
            "\n",
            "[97 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "UG63cQ4a7CDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label']=df['label'].map({0:'Cohesive',1:'Easyflow',2:'Freeflow'})"
      ],
      "metadata": {
        "id": "NG_Om_H77Gfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras"
      ],
      "metadata": {
        "id": "tccwMNYc7TPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an instance of the ImageDataGenerator for preprocessing the images\n",
        "data_gen = ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.vgg16.preprocess_input,\n",
        "    validation_split=0.3\n",
        ")\n",
        "\n",
        "# Create a train data generator from the train dataframe\n",
        "train_data_gen = data_gen.flow_from_dataframe(\n",
        "    dataframe=df,\n",
        "    x_col=\"path\",\n",
        "    y_col=\"label\",\n",
        "    target_size=(384, 384),\n",
        "    batch_size=32,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=True,\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Create a validation data generator from the validation dataframe\n",
        "val_data_gen = data_gen.flow_from_dataframe(\n",
        "    dataframe=df,\n",
        "    x_col=\"path\",\n",
        "    y_col=\"label\",\n",
        "    target_size=(384, 384),\n",
        "    batch_size=32,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=True,\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Load the pre-trained Vision Transformer model\n",
        "vision_transformer = tf.keras.applications.vgg16.VGG16(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=(384, 384, 3)\n",
        ")\n",
        "\n",
        "# Freeze the pre-trained layers\n",
        "for layer in vision_transformer.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add the classification head on top of the pre-trained layers\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        vision_transformer,\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(3, activation='softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Compile the model with categorical crossentropy loss and Adam optimizer\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    epochs=43,\n",
        "    validation_data=val_data_gen,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "y2UlhaAS7I6o",
        "outputId": "88a2b85a-6df5-4100-c7d8-dc6434a4896b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b34b53433b6b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create an instance of the ImageDataGenerator for preprocessing the images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m data_gen = ImageDataGenerator(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpreprocessing_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ImageDataGenerator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Keras-Preprocessing"
      ],
      "metadata": {
        "id": "1fazDpFVTulD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input"
      ],
      "metadata": {
        "id": "Wc4ywEk0Scvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = tf.keras.models.load_model('path/to/your/model.h5')\n",
        "\n",
        "# Load and preprocess the input image\n",
        "image_path = '/content/cmac_particle_flow/images/Cohesive/Span 60_19mm.jpg'\n",
        "input_size = (384, 384)  # Adjust this to match the input size of your model\n",
        "\n",
        "# Load the image\n",
        "image = load_img(image_path, target_size=input_size)\n",
        "image = img_to_array(image)\n",
        "# image /= 255.0  # Apply the scaling factor\n",
        "\n",
        "# Expand dimensions to create a batch of size 1\n",
        "image = np.expand_dims(image, axis=0)\n",
        "\n",
        "# Preprocess the input image\n",
        "image = preprocess_input(image)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(image)\n",
        "\n",
        "# Decode the predictions\n",
        "# (Adjust this part based on the output format of your model)\n",
        "predicted_class = np.argmax(predictions, axis=1)\n",
        "probability = np.max(predictions, axis=1)\n",
        "\n",
        "# Print the predicted class and probability\n",
        "print(f'Predicted Class: {predicted_class[0]}')\n",
        "print(f'Probability: {probability[0] * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "8C53-JiG7Olw",
        "outputId": "c067cbf1-dc7a-48bf-f14f-1e3c1c122159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-8bf3dd8ed4a2>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Expand dimensions to create a batch of size 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Preprocess the input image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install timm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlEES9XubDdP",
        "outputId": "2c87dd56-f90e-44aa-ff2d-2048823ce8a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.1+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0)\n",
            "Collecting huggingface-hub (from timm)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.14.1 safetensors-0.3.1 timm-0.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm"
      ],
      "metadata": {
        "id": "khECZTTXZybS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BHUeoSTSjUf3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}