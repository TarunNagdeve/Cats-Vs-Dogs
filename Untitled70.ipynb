{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsAtjmihIqnluOqu5WInWy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TarunNagdeve/Cats-Vs-Dogs/blob/master/Untitled70.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load image\n",
        "img = cv2.imread('/content/Screenshot (493).png')\n",
        "\n",
        "# Convert to grayscale\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Threshold image\n",
        "_, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
        "\n",
        "# Apply morphological operations\n",
        "kernel = np.ones((3,3), np.uint8)\n",
        "opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
        "sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
        "\n",
        "# Apply K-means clustering\n",
        "Z = img.reshape((-1,3))\n",
        "kmeans = KMeans(n_clusters=2)\n",
        "kmeans.fit(Z)\n",
        "\n",
        "# Determine which cluster corresponds to the agglomerates\n",
        "if kmeans.cluster_centers_[0][0] > kmeans.cluster_centers_[1][0]:\n",
        "    agglom_cluster = 0\n",
        "else:\n",
        "    agglom_cluster = 1\n",
        "\n",
        "# Label pixels as agglomerates or background based on clustering\n",
        "labels = np.zeros(img.shape[:2], np.uint8)\n",
        "for i in range(img.shape[0]):\n",
        "    for j in range(img.shape[1]):\n",
        "        if kmeans.labels_[i*img.shape[1]+j] == agglom_cluster:\n",
        "            labels[i,j] = 255\n",
        "\n",
        "# Apply morphological operations to remove noise\n",
        "kernel = np.ones((5,5), np.uint8)\n",
        "closing = cv2.morphologyEx(labels, cv2.MORPH_CLOSE, kernel)\n",
        "opening = cv2.morphologyEx(closing, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "# Find contours of agglomerates\n",
        "contours, hierarchy= cv2.findContours(opening, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Draw bounding boxes around agglomerates\n",
        "for cnt in contours:\n",
        "    x,y,w,h = cv2.boundingRect(cnt)\n",
        "    cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
        "\n",
        "# Show image with detected agglomerates\n",
        "cv2.imwrite('/content/detection.png', img)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbegPMj-1mC2",
        "outputId": "1d062a6b-64da-4184-d765-1cab388447b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Conv2D, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "# Download ResNet50 weights pre-trained on ImageNet\n",
        "weights_path = get_file(\n",
        "    'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
        "    'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
        "    cache_subdir='models',\n",
        "    cache_dir='./'\n",
        ")\n",
        "\n",
        "# Define the Mask-RCNN model with ResNet50 backbone\n",
        "def build_mask_rcnn_model():\n",
        "    # Load the ResNet50 base model without the top layers\n",
        "    resnet = ResNet50(include_top=False, weights=None)\n",
        "    \n",
        "    # Create the Mask-RCNN model architecture\n",
        "    inputs = Input(shape=(None, None, 3))\n",
        "    x = Conv2D(256, (3, 3), padding='same')(inputs)\n",
        "    outputs = resnet(x)\n",
        "    \n",
        "    model = Model(inputs, outputs)\n",
        "    \n",
        "    # Load the pre-trained ResNet50 weights\n",
        "    model.load_weights(weights_path)\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Load the Mask-RCNN model\n",
        "model = build_mask_rcnn_model()\n",
        "\n",
        "# Function to classify crystals and agglomerates in an image\n",
        "def classify_crystals_and_agglomerates(image_path, model):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Resize the image to match the input shape of the model\n",
        "    input_shape = model.input_shape[1:3]\n",
        "    image_resized = cv2.resize(image, input_shape)\n",
        "    \n",
        "    # Expand dimensions to create a batch of size 1\n",
        "    image_expanded = np.expand_dims(image_resized, axis=0)\n",
        "    \n",
        "    # Preprocess the image\n",
        "    image_preprocessed = tf.keras.applications.resnet50.preprocess_input(image_expanded)\n",
        "    \n",
        "    # Perform inference\n",
        "    predictions = model.predict(image_preprocessed)\n",
        "    \n",
        "    # Get the predicted class labels\n",
        "    class_labels = ['crystal', 'agglomerate']\n",
        "    class_indices = np.argmax(predictions, axis=3)\n",
        "    class_names = np.array([class_labels[idx] for idx in class_indices.flatten()])\n",
        "    \n",
        "    # Reshape the class names to match the original image size\n",
        "    class_names = class_names.reshape(class_indices.shape)\n",
        "    \n",
        "    return class_names\n",
        "\n",
        "# Provide the path to your image and perform inference\n",
        "image_path = 'path/to/your/image.jpg'\n",
        "class_names = classify_crystals_and_agglomerates(image_path, model)\n",
        "\n",
        "print(\"Class Names:\", class_names)\n"
      ],
      "metadata": {
        "id": "zAIdNsC-Mzdm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3084d1ec-7ef9-4609-ea08-e4820cc7fcd8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94653016/94653016 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-82b46c9042de>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Load the Mask-RCNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_mask_rcnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Function to classify crystals and agglomerates in an image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-82b46c9042de>\u001b[0m in \u001b[0;36mbuild_mask_rcnn_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 }:\n\u001b[0;32m--> 280\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    281\u001b[0m                         \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                         \u001b[0;34mf\"incompatible with the layer: expected axis {axis} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"resnet50\" (type Functional).\n\nInput 0 of layer \"conv1_conv\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (None, None, None, 256)\n\nCall arguments received by layer \"resnet50\" (type Functional):\n  • inputs=tf.Tensor(shape=(None, None, None, 256), dtype=float32)\n  • training=None\n  • mask=None"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "from tensorflow.keras.engine import saving\n",
        "\n",
        "def load_weights(self, filepath, by_name=False, exclude=None):\n",
        "    \"\"\"\n",
        "    Modified version of the corresponding Keras function with\n",
        "    the addition of multi-GPU support and the ability to exclude\n",
        "    some layers from loading.\n",
        "    \n",
        "    exclude: list of layer names to exclude\n",
        "    \"\"\"\n",
        "    if exclude:\n",
        "        by_name = True\n",
        "\n",
        "    if h5py is None:\n",
        "        raise ImportError('`load_weights` requires h5py.')\n",
        "    \n",
        "    f = h5py.File(filepath, mode='r')\n",
        "    \n",
        "    if 'layer_names' not in f.attrs and 'model_weights' in f:\n",
        "        f = f['model_weights']\n",
        "\n",
        "    # In multi-GPU training, we wrap the model. Get layers\n",
        "    # of the inner model because they have the weights.\n",
        "    keras_model = self.keras_model\n",
        "    layers = keras_model.inner_model.layers if hasattr(keras_model, \"inner_model\") else keras_model.layers\n",
        "\n",
        "    # Exclude some layers\n",
        "    if exclude:\n",
        "        layers = [l for l in layers if l.name not in exclude]\n",
        "\n",
        "    if by_name:\n",
        "        saving.load_weights_from_hdf5_group_by_name(f, layers)\n",
        "    else:\n",
        "        saving.load_weights_from_hdf5_group(f, layers)\n",
        "\n",
        "    if hasattr(f, 'close'):\n",
        "        f.close()\n",
        "\n",
        "    # Update the log directory\n",
        "    self.set_log_dir(filepath)\n"
      ],
      "metadata": {
        "id": "DDEuWZrX5J5K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        },
        "outputId": "8888c678-ea1b-42dd-eaca-98a99566f4a2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d9671eaa1cbd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaving\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.engine'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/maxfrei750/DeepParticleNet.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WOlcwxg5dvG",
        "outputId": "36653491-4f26-4881-b185-b9d8fe7f6fbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DeepParticleNet' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/DeepParticleNet/external"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_QTxEDs5dx9",
        "outputId": "f4c5ba20-a9d8-4cbb-b227-8097e1b0499c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DeepParticleNet/external/CLR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1X0_YXh5d01",
        "outputId": "f1947721-d627-4488-c5ec-2e29e00e610e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CLR'...\n",
            "remote: Enumerating objects: 262, done.\u001b[K\n",
            "remote: Total 262 (delta 0), reused 0 (delta 0), pack-reused 262\u001b[K\n",
            "Receiving objects: 100% (262/262), 1.37 MiB | 15.59 MiB/s, done.\n",
            "Resolving deltas: 100% (95/95), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/DeepParticleNet/external\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWUBJNJy5d3-",
        "outputId": "2d6b68d8-69bf-422c-b7a9-6808938b803b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DeepParticleNet/external\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/matterport/Mask_RCNN.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFawKmLO5d7W",
        "outputId": "9dba9ba6-5a37-4f71-e1e3-1e166e55cbb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Mask_RCNN'...\n",
            "remote: Enumerating objects: 956, done.\u001b[K\n",
            "remote: Total 956 (delta 0), reused 0 (delta 0), pack-reused 956\u001b[K\n",
            "Receiving objects: 100% (956/956), 137.67 MiB | 28.99 MiB/s, done.\n",
            "Resolving deltas: 100% (558/558), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WDhrmIGv5d9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gPOJLKW_5eAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KsEDOZaR5eEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4A_luw915eG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y8BSI79S5cIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LmRMhIW4H5qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kcgoKZzsH6Km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load image\n",
        "image = cv2.imread('/content/Screenshot (493).png')\n",
        "\n",
        "# Convert image to grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Apply Gaussian blur to reduce noise\n",
        "blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
        "\n",
        "# Apply adaptive thresholding to binarize the image\n",
        "thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
        "\n",
        "# Apply morphological operations to remove noise and fill gaps\n",
        "kernel = np.ones((3,3), np.uint8)\n",
        "closing = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=4)\n",
        "opening = cv2.morphologyEx(closing, cv2.MORPH_OPEN, kernel, iterations=4)\n",
        "\n",
        "# Find contours in the image\n",
        "contours, hierarchy = cv2.findContours(opening, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Loop over the contours and check for agglomerates\n",
        "for contour in contours:\n",
        "    # Compute the area and perimeter of the contour\n",
        "    area = cv2.contourArea(contour)\n",
        "    perimeter = cv2.arcLength(contour, True)\n",
        "    \n",
        "    # Compute the circularity of the contour\n",
        "    circularity = 4 * np.pi * area / (perimeter ** 2)\n",
        "    \n",
        "    # If the circularity is below a certain threshold, it is likely an agglomerate\n",
        "    if circularity < 0.5:\n",
        "        # Draw a bounding box around the contour\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
        "\n",
        "# Display the image with the detected agglomerates\n",
        "cv2.imwrite('/content/detection2.png', img)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "1OxYCf5R12t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Load image\n",
        "img = cv2.imread('/content/Screenshot (493).png')\n",
        "\n",
        "# Convert to grayscale\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Threshold image\n",
        "_, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
        "\n",
        "# Apply morphological operations to remove noise\n",
        "kernel = np.ones((5,5), np.uint8)\n",
        "opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
        "\n",
        "# Flatten pixel values into feature vectors\n",
        "X = opening.reshape(-1, 1)\n",
        "\n",
        "# Apply isolation forest anomaly detection\n",
        "clf = IsolationForest(contamination='auto')\n",
        "clf.fit(X)\n",
        "\n",
        "# Determine threshold for anomaly scores\n",
        "scores = clf.decision_function(X)\n",
        "thresh = np.percentile(scores, 5)\n",
        "\n",
        "# Label pixels as agglomerates or background based on anomaly score\n",
        "labels = np.zeros(img.shape[:2], np.uint8)\n",
        "for i in range(img.shape[0]):\n",
        "    for j in range(img.shape[1]):\n",
        "        score = clf.decision_function(np.array([opening[i,j]]).reshape(1, -1))[0]\n",
        "        if score < thresh:\n",
        "            labels[i,j] = 255\n",
        "\n",
        "# Apply morphological operations to remove noise\n",
        "kernel = np.ones((5,5), np.uint8)\n",
        "closing = cv2.morphologyEx(labels, cv2.MORPH_CLOSE, kernel)\n",
        "opening = cv2.morphologyEx(closing, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "# Find contours of agglomerates\n",
        "contours, hierarchy = cv2.findContours(opening, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Draw bounding boxes around agglomerates\n",
        "for cnt in contours:\n",
        "    x,y,w,h = cv2.boundingRect(cnt)\n",
        "    cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
        "\n",
        "# Show image with detected agglomerates\n",
        "cv2.imwrite('/content/detection3.png', img)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "-bNKP3F3QU7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MRW-Code/cmac_particle_flow.git\n"
      ],
      "metadata": {
        "id": "FMJJx8LARdn-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e098c51-2576-4cad-bd1e-5fd2080dbb5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cmac_particle_flow'...\n",
            "remote: Enumerating objects: 4458, done.\u001b[K\n",
            "remote: Counting objects: 100% (361/361), done.\u001b[K\n",
            "remote: Compressing objects: 100% (181/181), done.\u001b[K\n",
            "remote: Total 4458 (delta 242), reused 295 (delta 178), pack-reused 4097\u001b[K\n",
            "Receiving objects: 100% (4458/4458), 1.72 GiB | 26.50 MiB/s, done.\n",
            "Resolving deltas: 100% (494/494), done.\n",
            "Updating files: 100% (241/241), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "def preprocess_image(image_path):\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Split the image into 49 equal parts\n",
        "    rows = 7\n",
        "    cols = 7\n",
        "    width, height = img.size\n",
        "    tile_size = (width//cols, height//rows)\n",
        "\n",
        "    tiles = []\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            left = j * tile_size[0]\n",
        "            upper = i * tile_size[1]\n",
        "            right = left + tile_size[0]\n",
        "            lower = upper + tile_size[1]\n",
        "            tile = img.crop((left, upper, right, lower))\n",
        "            tiles.append(tile)\n",
        "\n",
        "    # Center crop and resize each tile\n",
        "    size = (384, 384)\n",
        "    for i in range(len(tiles)):\n",
        "        # Center crop\n",
        "        w, h = tiles[i].size\n",
        "        left = (w - size[0]) / 2\n",
        "        top = (h - size[1]) / 2\n",
        "        right = (w + size[0]) / 2\n",
        "        bottom = (h + size[1]) / 2\n",
        "        tiles[i] = tiles[i].crop((left, top, right, bottom))\n",
        "        \n",
        "        # Resize\n",
        "        tiles[i] = tiles[i].resize(size)\n",
        "\n",
        "    # Convert the image tiles to numpy arrays\n",
        "    image_data = []\n",
        "    for tile in tiles:\n",
        "        img_array = np.array(tile)\n",
        "        image_data.append(img_array)\n",
        "\n",
        "    # Convert the list of numpy arrays to a single numpy array\n",
        "    image_data = np.stack(image_data)\n",
        "\n",
        "    # Normalize the image data\n",
        "    image_data = image_data.astype('float32') / 255.\n",
        "\n",
        "    return image_data"
      ],
      "metadata": {
        "id": "O5Kv5SIadeo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_image('/content/cmac_particle_flow/images/Cohesive/Calcium Carbonate (40%) - binary.jpg')"
      ],
      "metadata": {
        "id": "KUnUjlYCeIFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_data(images):\n",
        "    image_data = []\n",
        "    labels = []\n",
        "\n",
        "    for i, row in images.iterrows():\n",
        "        image_path = row['path']\n",
        "        label = row['label']\n",
        "        # image = preprocess_image(image_path)\n",
        "        image_data.append(image)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Convert the image data to a numpy array\n",
        "    image_data = np.array(image_data)\n",
        "\n",
        "    # Convert the labels to a numpy array\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return image_data, labels"
      ],
      "metadata": {
        "id": "TeryBdRjeR72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qIVBLOiB5N7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cohesive='/content/cmac_particle_flow/images/Cohesive'\n",
        "easyflow='/content/cmac_particle_flow/images/Easyflowing'\n",
        "freeflow='/content/cmac_particle_flow/images/Freeflowing'"
      ],
      "metadata": {
        "id": "TwwZwdBFfuT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths=[]\n",
        "labels=[]\n",
        "import os\n",
        "\n",
        "for i in os.listdir(cohesive):\n",
        "  image_paths.append(os.path.join(cohesive,i))\n",
        "\n"
      ],
      "metadata": {
        "id": "nBoAjgsckrNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(image_paths)):\n",
        "  labels.append(0)"
      ],
      "metadata": {
        "id": "g85n5rs4lGp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in os.listdir(easyflow):\n",
        "  image_paths.append(os.path.join(easyflow,i))"
      ],
      "metadata": {
        "id": "6rXCPkeslY9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(34):\n",
        "  labels.append(1)"
      ],
      "metadata": {
        "id": "G4viPbOIl6Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in os.listdir(freeflow):\n",
        "  image_paths.append(os.path.join(freeflow,i))"
      ],
      "metadata": {
        "id": "H3zLL0BYmUeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(33):\n",
        "  labels.append(2)"
      ],
      "metadata": {
        "id": "ecXy0pJUml2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_data = {'path': image_paths, 'label': labels}\n",
        "df = pd.DataFrame(data=image_data)"
      ],
      "metadata": {
        "id": "PjX0hARDmyXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6w80STwmzvu",
        "outputId": "5f304646-b423-49f8-9a41-f14fba656b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 path  label\n",
            "0   /content/cmac_particle_flow/images/Cohesive/Ce...      0\n",
            "1   /content/cmac_particle_flow/images/Cohesive/Sp...      0\n",
            "2   /content/cmac_particle_flow/images/Cohesive/Pa...      0\n",
            "3   /content/cmac_particle_flow/images/Cohesive/Ca...      0\n",
            "4   /content/cmac_particle_flow/images/Cohesive/Pa...      0\n",
            "..                                                ...    ...\n",
            "92  /content/cmac_particle_flow/images/Freeflowing...      2\n",
            "93  /content/cmac_particle_flow/images/Freeflowing...      2\n",
            "94  /content/cmac_particle_flow/images/Freeflowing...      2\n",
            "95  /content/cmac_particle_flow/images/Freeflowing...      2\n",
            "96  /content/cmac_particle_flow/images/Freeflowing...      2\n",
            "\n",
            "[97 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "gNGuBlttnOa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label']=df['label'].map({0:'Cohesive',1:'Easyflow',2:'Freeflow'})"
      ],
      "metadata": {
        "id": "bE_XRdf_919C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.applications import EfficientNetB0\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# Split the dataframe into train and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an instance of the ImageDataGenerator for preprocessing the images\n",
        "data_gen = ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.vgg16.preprocess_input,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Create a train data generator from the train dataframe\n",
        "train_data_gen = data_gen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col=\"path\",\n",
        "    y_col=\"label\",\n",
        "    target_size=(384, 384),\n",
        "    batch_size=32,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=True,\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Create a validation data generator from the validation dataframe\n",
        "val_data_gen = data_gen.flow_from_dataframe(\n",
        "    dataframe=val_df,\n",
        "    x_col=\"path\",\n",
        "    y_col=\"label\",\n",
        "    target_size=(384, 384),\n",
        "    batch_size=32,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=True,\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Load the pre-trained Vision Transformer model\n",
        "vision_transformer = tf.keras.applications.vgg16.VGG16(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=(384, 384, 3)\n",
        ")\n",
        "\n",
        "# Freeze the pre-trained layers\n",
        "for layer in vision_transformer.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add the classification head on top of the pre-trained layers\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        vision_transformer,\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(3, activation='softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Compile the model with categorical crossentropy loss and Adam optimizer\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    epochs=10,\n",
        "    validation_data=val_data_gen,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NDUChKF58if",
        "outputId": "be213dd1-1751-421c-a810-978bcc8bf9c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 62 validated image filenames belonging to 3 classes.\n",
            "Found 4 validated image filenames belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 138s 68s/step - loss: 52.2803 - accuracy: 0.3226 - val_loss: 56.7491 - val_accuracy: 0.5000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 129s 61s/step - loss: 65.2644 - accuracy: 0.4032 - val_loss: 38.7683 - val_accuracy: 0.5000\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 128s 62s/step - loss: 30.0645 - accuracy: 0.5806 - val_loss: 19.4375 - val_accuracy: 0.5000\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 129s 64s/step - loss: 20.7797 - accuracy: 0.6129 - val_loss: 50.0738 - val_accuracy: 0.2500\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 128s 65s/step - loss: 26.2192 - accuracy: 0.6290 - val_loss: 42.4098 - val_accuracy: 0.2500\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 128s 64s/step - loss: 16.0072 - accuracy: 0.6452 - val_loss: 20.9956 - val_accuracy: 0.2500\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 126s 59s/step - loss: 7.0376 - accuracy: 0.7903 - val_loss: 17.6163 - val_accuracy: 0.5000\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 128s 65s/step - loss: 10.6410 - accuracy: 0.7581 - val_loss: 18.0587 - val_accuracy: 0.5000\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 126s 61s/step - loss: 11.2770 - accuracy: 0.7419 - val_loss: 21.5175 - val_accuracy: 0.2500\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 127s 61s/step - loss: 3.8830 - accuracy: 0.8871 - val_loss: 23.3011 - val_accuracy: 0.2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12QwaBHKUXXJ",
        "outputId": "9830c2a2-9801-4185-8f2e-ea56a732dbf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from keras.optimizers import SGD\n",
        "from keras.applications import EfficientNetB0\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataframe containing image data\n",
        "data = df\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an instance of the image data generator and rescale pixel values\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create training data generator\n",
        "train_generator = datagen.flow_from_dataframe(\n",
        "        dataframe=train_data,\n",
        "        x_col='path',\n",
        "        y_col='label',\n",
        "        target_size=(384, 384),\n",
        "        batch_size=16,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# Create validation data generator\n",
        "val_generator = datagen.flow_from_dataframe(\n",
        "        dataframe=val_data,\n",
        "        x_col='path',\n",
        "        y_col='label',\n",
        "        target_size=(384, 384),\n",
        "        batch_size=16,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# Load the pre-trained model without the top layers\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False)\n",
        "\n",
        "# Freeze the pre-trained layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add top layers to the pre-trained model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with a learning rate of 0.001 and momentum of 0.9\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model for 10 epochs\n",
        "history = model.fit(train_generator, validation_data=val_generator, epochs=10)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "model.evaluate(val_generator)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMM9qGRkYApp",
        "outputId": "2ca7fad9-4cc5-4a74-94ce-a6b752942030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 77 validated image filenames belonging to 3 classes.\n",
            "Found 20 validated image filenames belonging to 3 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16705208/16705208 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "5/5 [==============================] - 55s 9s/step - loss: 1.1249 - accuracy: 0.3506 - val_loss: 1.0969 - val_accuracy: 0.3500\n",
            "Epoch 2/10\n",
            "5/5 [==============================] - 37s 7s/step - loss: 1.0727 - accuracy: 0.4545 - val_loss: 1.0967 - val_accuracy: 0.3000\n",
            "Epoch 3/10\n",
            "5/5 [==============================] - 40s 8s/step - loss: 1.1298 - accuracy: 0.3506 - val_loss: 1.0963 - val_accuracy: 0.3500\n",
            "Epoch 4/10\n",
            "5/5 [==============================] - 41s 8s/step - loss: 1.0822 - accuracy: 0.4286 - val_loss: 1.0962 - val_accuracy: 0.3500\n",
            "Epoch 5/10\n",
            "5/5 [==============================] - 40s 8s/step - loss: 1.1184 - accuracy: 0.2727 - val_loss: 1.0962 - val_accuracy: 0.3500\n",
            "Epoch 6/10\n",
            "5/5 [==============================] - 38s 8s/step - loss: 1.1281 - accuracy: 0.3377 - val_loss: 1.0965 - val_accuracy: 0.3500\n",
            "Epoch 7/10\n",
            "5/5 [==============================] - 40s 8s/step - loss: 1.1176 - accuracy: 0.3377 - val_loss: 1.0967 - val_accuracy: 0.3500\n",
            "Epoch 8/10\n",
            "5/5 [==============================] - 40s 8s/step - loss: 1.1233 - accuracy: 0.3247 - val_loss: 1.0965 - val_accuracy: 0.3500\n",
            "Epoch 9/10\n",
            "5/5 [==============================] - 39s 8s/step - loss: 1.1186 - accuracy: 0.2987 - val_loss: 1.0963 - val_accuracy: 0.3500\n",
            "Epoch 10/10\n",
            "5/5 [==============================] - 41s 8s/step - loss: 1.1361 - accuracy: 0.3377 - val_loss: 1.0962 - val_accuracy: 0.3500\n",
            "2/2 [==============================] - 8s 718ms/step - loss: 1.0962 - accuracy: 0.3500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0962426662445068, 0.3499999940395355]"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoModel, TFBertModel, TFAutoModel \n",
        "vision_transformer = TFAutoModel.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "# Freeze all layers of the pre-trained model\n",
        "for layer in vision_transformer.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Unfreeze the last few layers for fine-tuning\n",
        "for layer in vision_transformer.layers[-10:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(lr=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_generator, epochs=10, validation_data=val_generator)"
      ],
      "metadata": {
        "id": "9vnYhT2259DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g5RpPPqT59YJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S-i4bKN759u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W4UrpLoV5-LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h7--QSN75-mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = df.sample(frac=0.8, random_state=1)\n",
        "val_df = df.drop(train_df.index)"
      ],
      "metadata": {
        "id": "eDv7kofFnSbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = load_image_data(train_df)\n",
        "x_val, y_val = load_image_data(val_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7zPU1BwnUIj",
        "outputId": "b9101b4e-a6f0-4a7d-99c1-2a2aec89b1b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-66-89fd1c0a353c>:13: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  image_data = np.array(image_data)\n",
            "<ipython-input-66-89fd1c0a353c>:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  image_data = np.array(image_data)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0]"
      ],
      "metadata": {
        "id": "-A5BabEI1PGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "for i in range(49):\n",
        "    img = Image.open(df.loc[0, 'path'])\n",
        "    width, height = img.size\n",
        "    x = (width - 384) // 2\n",
        "    y = (height - 384) // 2\n",
        "    img = img.crop((x, y, x + 384, y + 384))\n",
        "    img = np.array(img.resize((384, 384)))\n",
        "    images.append(img)\n",
        "\n",
        "# Flatten the list of images into a single array\n",
        "x = np.array(images).reshape((1, -1, 384, 384, 3))\n",
        "\n",
        "# Load the pre-trained model\n",
        "base_model = keras.applications.vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(384, 384, 3))\n",
        "\n",
        "# Freeze the layers in the base model\n",
        "for layer in base_model.layers[:-4]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Build the transfer learning model\n",
        "model = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.Rescaling(1./255),\n",
        "    layers.TimeDistributed(base_model),\n",
        "    layers.TimeDistributed(layers.Flatten()),\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(units=128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(units=3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "QiqDz5nD0OBj",
        "outputId": "52686624-e44f-4aeb-8ca2-63a3ef577d5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-6d6882214ba7>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Flatten the list of images into a single array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Load the pre-trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 7225344 into shape (1,newaxis,384,384,3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xH6i6cPk0dQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Load the image\n",
        "img = Image.open('/content/cmac_particle_flow/images/Cohesive/Caffeine_15mm.jpg')\n",
        "\n",
        "# Define the crop size\n",
        "crop_size = 384\n",
        "\n",
        "# Split the image into 49 parts\n",
        "width, height = img.size\n",
        "x_stride = (width - crop_size) // 6\n",
        "y_stride = (height - crop_size) // 6\n",
        "images = []\n",
        "for x in range(7):\n",
        "    for y in range(7):\n",
        "        x_pos = x_stride * x\n",
        "        y_pos = y_stride * y\n",
        "        cropped_img = img.crop((x_pos, y_pos, x_pos + crop_size, y_pos + crop_size))\n",
        "        images.append(cropped_img)\n",
        "\n",
        "# Center crop each image to 384x384\n",
        "center_cropped_images = []\n",
        "for image in images:\n",
        "    width, height = image.size\n",
        "    left = (width - crop_size) / 2\n",
        "    top = (height - crop_size) / 2\n",
        "    right = (width + crop_size) / 2\n",
        "    bottom = (height + crop_size) / 2\n",
        "    center_cropped_images.append(image.crop((left, top, right, bottom)))"
      ],
      "metadata": {
        "id": "-2oghfZm0daU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "center_cropped_images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLKVBAj9263J",
        "outputId": "5588dd46-9a05-4969-a779-79c4f027ed3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<PIL.Image.Image image mode=L size=384x384 at 0x7FB5881EBDC0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356AC80>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356A2C0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356B4F0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356B880>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB583569FF0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB583568970>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB583569240>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB5835697E0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356A1A0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB583568EB0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356B190>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356A5F0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356A380>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356A590>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356A740>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB583568040>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356A980>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356A050>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356B3D0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356B700>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356A650>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356A6B0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356B130>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356AFE0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356B9A0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356B400>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB583568E20>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB583568220>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB583568250>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB583568490>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB583568100>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB5835680A0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB583569A20>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB5835693F0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356A950>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356ABC0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356B7F0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB583569BD0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356AFB0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356B9D0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB583568CD0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB583568AC0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58356A4A0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB583569810>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB5835682B0>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB583569090>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB583569B40>,\n",
              " <PIL.Image.Image image mode=L size=384x384 at 0x7FB58857C5B0>]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2KYQ9a62266W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "khubTNAH269Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "24qepoVQ27A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u5dg9qkV27Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kA32z3V_27HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transfer learning model\n",
        "base_model = keras.applications.vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(384, 384, 3))\n",
        "\n",
        "for layer in base_model.layers[:-4]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.Rescaling(1./255),\n",
        "    base_model,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(units=128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(units=3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "VF7-eb-YnXUF",
        "outputId": "7c761d97-bd1a-4a44-c194-a90e09c37c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-ba6294d42e54>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type Image)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-qjuw3aZyhYO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}