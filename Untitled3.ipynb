{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORYMgDuXTK3Fe5ptskO9N6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TarunNagdeve/Cats-Vs-Dogs/blob/master/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TetghydNSwp0"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "# Create a Streamlit app\n",
        "st.title(\"Zip File Upload and Save Example\")\n",
        "\n",
        "# Use st.file_uploader() to let the user upload a zip file\n",
        "zip_file = st.file_uploader(\"Upload a zip file\", type=[\"zip\"])\n",
        "\n",
        "# Check if a zip file was uploaded\n",
        "if zip_file:\n",
        "    st.write(\"You uploaded a zip file:\", zip_file)\n",
        "\n",
        "    # You can choose a destination directory where you want to save the extracted contents\n",
        "    destination_directory = \"path/to/your/destination\"\n",
        "\n",
        "    # Check if the destination directory exists, and create it if it doesn't\n",
        "    if not st.file_uploader:\n",
        "        os.makedirs(destination_directory)\n",
        "\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(destination_directory)\n",
        "\n",
        "    st.write(\"Zip file extracted and saved to the destination:\", destination_directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.document_store.memory import InMemoryDocumentStore\n",
        "from haystack.retriever.dense import DensePassageRetriever\n",
        "from haystack.pipeline import DocumentSearchPipeline\n",
        "\n",
        "# Initialize the Document Store\n",
        "document_store = InMemoryDocumentStore()\n",
        "\n",
        "# Assuming you have a DataFrame called 'df' with columns 'indexes', 'data', and 'docs'\n",
        "# You can add your documents to the Document Store\n",
        "for index, row in df.iterrows():\n",
        "    document_store.write_documents(\n",
        "        [{\"text\": row['data'], \"meta\": {\"doc_id\": row['indexes'], \"doc_name\": row['docs']}}\n",
        "    )\n",
        "\n",
        "# Initialize the Dense Passage Retriever (DPR)\n",
        "retriever = DensePassageRetriever(\n",
        "    document_store=document_store,\n",
        "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
        "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
        "    use_gpu=False,  # Set to True if you have a GPU\n",
        ")\n",
        "\n",
        "# Create a Document Search Pipeline\n",
        "pipeline = DocumentSearchPipeline(retriever, document_store)\n",
        "\n",
        "# Define an input text for retrieval\n",
        "input_text = \"Your input text goes here.\"\n",
        "\n",
        "# Use the pipeline to retrieve the top 5 matching texts\n",
        "results = pipeline.run(query=input_text, top_k_retriever=5)\n",
        "\n",
        "# Print the retrieved documents\n",
        "for hit in results['documents']:\n",
        "    print(f\"Document ID: {hit.meta['doc_id']}, Document Name: {hit.meta['doc_name']}\")\n",
        "    print(f\"Text: {hit.text}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OSqh1LmmMTp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.document_store.faiss import FAISSDocumentStore\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from haystack.schema import Document\n",
        "\n",
        "# Initialize the FAISS document store\n",
        "document_store = FAISSDocumentStore(sql_url=\"sqlite:///my_faiss_store.db\", index=\"hnsw_flat\")\n",
        "\n",
        "# Initialize a SentenceTransformer model\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Generate embeddings for your data and store them in FAISS\n",
        "for entry in data_to_index:\n",
        "    content = entry[\"data\"]  # Extract the text content\n",
        "    embedding = model.encode(content)  # Generate the embedding\n",
        "\n",
        "    # Create a FAISSDocument with 'content' and 'meta' fields\n",
        "    document = Document(content=content, meta={\"docs\": entry[\"docs\"], \"indexes\": entry[\"indexes\"]})\n",
        "\n",
        "    # Store the document with the embedding in FAISS\n",
        "    document_store.write_documents([document], index=\"hnsw_flat\", update_existing=True)\n"
      ],
      "metadata": {
        "id": "R4QZPaqXxF28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.document_store.faiss import FAISSDocumentStore\n",
        "\n",
        "# Initialize the FAISS document store\n",
        "document_store = FAISSDocumentStore(sql_url=\"sqlite:///my_faiss_store.db\", index=\"hnsw_flat\")\n",
        "\n",
        "# Your dataset\n",
        "data_to_index = [\n",
        "    {\"data\": \"Text from document 1, section A\", \"docs\": \"document1\", \"indexes\": \"A\"},\n",
        "    {\"data\": \"Text from document 1, section B\", \"docs\": \"document1\", \"indexes\": \"B\"},\n",
        "    {\"data\": \"Text from document 2, section A\", \"docs\": \"document2\", \"indexes\": \"A\"},\n",
        "    # Add more data entries as needed\n",
        "]\n",
        "\n",
        "# Store the data in FAISS\n",
        "for entry in data_to_index:\n",
        "    document_store.write_documents([{\"text\": entry[\"data\"], \"meta\": {\"docs\": entry[\"docs\"], \"indexes\": entry[\"indexes\"]}],\n",
        "                                   update_existing=True)\n"
      ],
      "metadata": {
        "id": "pf1x5Rr2yjb9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}