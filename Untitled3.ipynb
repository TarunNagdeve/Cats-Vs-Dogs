{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjFROA7HipisIL+hk4cQdU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TarunNagdeve/Cats-Vs-Dogs/blob/master/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.pipeline import DocumentSearchPipeline\n",
        "\n",
        "# Assuming you have initialized a retriever and a reader\n",
        "retriever = DensePassageRetriever(document_store=loaded_document_store)\n",
        "reader = FARMReader(model_name_or_path=\"your_reader_model\")\n",
        "\n",
        "# Create a Document Search Pipeline that combines the retriever and reader\n",
        "pipeline = DocumentSearchPipeline(retriever, reader)\n",
        "\n",
        "# Define an input query or question\n",
        "query = \"Your query goes here.\"\n",
        "\n",
        "# Execute the pipeline to retrieve and extract answers\n",
        "result = pipeline.run(query=query, top_k_retriever=5)  # Adjust top_k_retriever as needed\n",
        "\n",
        "# Print or process the extracted answers\n",
        "if result['answers']:\n",
        "    for answer in result['answers']:\n",
        "        print(f\"Extracted Answer: {answer['answer']}\")\n",
        "else:\n",
        "    print(\"No answer found.\")\n"
      ],
      "metadata": {
        "id": "rBgTvwe5oBlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.pipeline import DocumentSearchPipeline\n",
        "\n",
        "# Assuming you have initialized a retriever and a reader\n",
        "retriever = DensePassageRetriever(document_store=loaded_document_store)\n",
        "reader = FARMReader(model_name_or_path=\"your_reader_model\")\n",
        "\n",
        "# Create a Document Search Pipeline that combines the retriever and reader\n",
        "pipeline = DocumentSearchPipeline(retriever, reader)\n",
        "\n",
        "# Define an input query or question\n",
        "query = \"Your query goes here.\"\n",
        "\n",
        "# Execute the pipeline to retrieve and extract answers\n",
        "result = pipeline.run(query=query, top_k_retriever=5)  # Adjust top_k_retriever as needed\n",
        "\n",
        "# Check if answers are found\n",
        "if result and \"answers\" in result:\n",
        "    answers = result[\"answers\"]\n",
        "    if answers:\n",
        "        for answer in answers:\n",
        "            print(f\"Extracted Answer: {answer['answer']} (Confidence: {answer['score']})\")\n",
        "    else:\n",
        "        print(\"No answer found.\")\n",
        "else:\n",
        "    print(\"No answer found or no 'answers' object in the result.\")\n"
      ],
      "metadata": {
        "id": "BPWlMNnrQeIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def number_particles(image, green_boxes, red_boxes):\n",
        "    # Combine both green and red bounding boxes\n",
        "    all_boxes = green_boxes + red_boxes\n",
        "    all_boxes.sort(key=lambda box: (box[1], box[0]))  # Sort by top-left corner coordinates\n",
        "\n",
        "    # Initialize a counter for numbering particles\n",
        "    particle_number = 1\n",
        "\n",
        "    # Draw bounding boxes and number each particle\n",
        "    for box in all_boxes:\n",
        "        x, y, w, h = box\n",
        "        center_x = x + w // 2\n",
        "        center_y = y + h // 2\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        font_scale = 0.5\n",
        "        font_color = (0, 0, 0)  # Black color for text\n",
        "\n",
        "        if box in green_boxes:\n",
        "            box_color = (0, 255, 0)  # Green bounding box\n",
        "        else:\n",
        "            box_color = (0, 0, 255)  # Red bounding box\n",
        "\n",
        "        image = cv2.rectangle(image, (x, y), (x + w, y + h), box_color, 2)\n",
        "        image = cv2.putText(image, str(particle_number), (center_x, center_y), font, font_scale, font_color, 2)\n",
        "        particle_number += 1\n",
        "\n",
        "    return image\n",
        "\n",
        "# Example usage:\n",
        "# image = cv2.imread(\"your_image.jpg\")\n",
        "# green_boxes = [(x1, y1, width1, height1), (x2, y2, width2, height2)]\n",
        "# red_boxes = [(x3, y3, width3, height3), (x4, y4, width4, height4)]\n",
        "# result_image = number_particles(image, green_boxes, red_boxes)\n",
        "# cv2.imshow(\"Numbered Particles\", result_image)\n",
        "# cv2.waitKey(0)\n",
        "# cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "XlmyzJhdoC3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.document_store.faiss import FAISSDocumentStore\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from haystack.schema import Document\n",
        "\n",
        "# Initialize the FAISS document store\n",
        "document_store = FAISSDocumentStore(sql_url=\"sqlite:///my_faiss_store.db\", index=\"hnsw_flat\")\n",
        "\n",
        "# Initialize a SentenceTransformer model\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Generate embeddings for your data and store them in FAISS\n",
        "for entry in data_to_index:\n",
        "    content = entry[\"data\"]  # Extract the text content\n",
        "    embedding = model.encode(content)  # Generate the embedding\n",
        "\n",
        "    # Create a FAISSDocument with 'content' and 'meta' fields\n",
        "    document = Document(content=content, meta={\"docs\": entry[\"docs\"], \"indexes\": entry[\"indexes\"]})\n",
        "\n",
        "    # Store the document with the embedding in FAISS\n",
        "    document_store.write_documents([document], index=\"hnsw_flat\", update_existing=True)\n"
      ],
      "metadata": {
        "id": "R4QZPaqXxF28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.document_store.faiss import FAISSDocumentStore\n",
        "\n",
        "# Initialize the FAISS document store\n",
        "document_store = FAISSDocumentStore(sql_url=\"sqlite:///my_faiss_store.db\", index=\"hnsw_flat\")\n",
        "\n",
        "# Your dataset\n",
        "data_to_index = [\n",
        "    {\"data\": \"Text from document 1, section A\", \"docs\": \"document1\", \"indexes\": \"A\"},\n",
        "    {\"data\": \"Text from document 1, section B\", \"docs\": \"document1\", \"indexes\": \"B\"},\n",
        "    {\"data\": \"Text from document 2, section A\", \"docs\": \"document2\", \"indexes\": \"A\"},\n",
        "    # Add more data entries as needed\n",
        "]\n",
        "\n",
        "# Store the data in FAISS\n",
        "for entry in data_to_index:\n",
        "    document_store.write_documents([{\"text\": entry[\"data\"], \"meta\": {\"docs\": entry[\"docs\"], \"indexes\": entry[\"indexes\"]}],\n",
        "                                   update_existing=True)\n"
      ],
      "metadata": {
        "id": "pf1x5Rr2yjb9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}