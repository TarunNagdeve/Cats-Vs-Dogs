{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeTanRT1egVfhqNQfLXUJ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TarunNagdeve/Cats-Vs-Dogs/blob/master/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TetghydNSwp0"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "# Create a Streamlit app\n",
        "st.title(\"Zip File Upload and Save Example\")\n",
        "\n",
        "# Use st.file_uploader() to let the user upload a zip file\n",
        "zip_file = st.file_uploader(\"Upload a zip file\", type=[\"zip\"])\n",
        "\n",
        "# Check if a zip file was uploaded\n",
        "if zip_file:\n",
        "    st.write(\"You uploaded a zip file:\", zip_file)\n",
        "\n",
        "    # You can choose a destination directory where you want to save the extracted contents\n",
        "    destination_directory = \"path/to/your/destination\"\n",
        "\n",
        "    # Check if the destination directory exists, and create it if it doesn't\n",
        "    if not st.file_uploader:\n",
        "        os.makedirs(destination_directory)\n",
        "\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(destination_directory)\n",
        "\n",
        "    st.write(\"Zip file extracted and saved to the destination:\", destination_directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.document_store.memory import InMemoryDocumentStore\n",
        "from haystack.retriever.dense import DensePassageRetriever\n",
        "from haystack.pipeline import DocumentSearchPipeline\n",
        "\n",
        "# Initialize the Document Store\n",
        "document_store = InMemoryDocumentStore()\n",
        "\n",
        "# Assuming you have a DataFrame called 'df' with columns 'indexes', 'data', and 'docs'\n",
        "# You can add your documents to the Document Store\n",
        "for index, row in df.iterrows():\n",
        "    document_store.write_documents(\n",
        "        [{\"text\": row['data'], \"meta\": {\"doc_id\": row['indexes'], \"doc_name\": row['docs']}}\n",
        "    )\n",
        "\n",
        "# Initialize the Dense Passage Retriever (DPR)\n",
        "retriever = DensePassageRetriever(\n",
        "    document_store=document_store,\n",
        "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
        "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
        "    use_gpu=False,  # Set to True if you have a GPU\n",
        ")\n",
        "\n",
        "# Create a Document Search Pipeline\n",
        "pipeline = DocumentSearchPipeline(retriever, document_store)\n",
        "\n",
        "# Define an input text for retrieval\n",
        "input_text = \"Your input text goes here.\"\n",
        "\n",
        "# Use the pipeline to retrieve the top 5 matching texts\n",
        "results = pipeline.run(query=input_text, top_k_retriever=5)\n",
        "\n",
        "# Print the retrieved documents\n",
        "for hit in results['documents']:\n",
        "    print(f\"Document ID: {hit.meta['doc_id']}, Document Name: {hit.meta['doc_name']}\")\n",
        "    print(f\"Text: {hit.text}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OSqh1LmmMTp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retriever:\n",
        "Responsibility: The retriever is responsible for finding the most relevant documents (or passages) in a large collection of text based on a query or input question.\n",
        "Technique: It uses techniques like dense vector similarity search to match the query against precomputed document embeddings, ranking the documents by relevance.\n",
        "Output: The output of the retriever is a set of documents or passages ranked by relevance to the input query.\n",
        "Use Case: The retriever is the first stage in a pipeline and is typically used for narrowing down the search space by identifying relevant documents. It's highly efficient for this task.\n",
        "Reader:\n",
        "Responsibility: The reader's role is to extract answers or relevant information from a set of documents retrieved by the retriever. It attempts to understand and summarize the content in those documents to provide direct answers to questions.\n",
        "Technique: It uses techniques like machine learning models (e.g., BERT-based models) for text comprehension and question-answering to find specific answers within the text.\n",
        "Output: The output of the reader is one or more answers along with the corresponding context from the documents.\n",
        "Use Case: The reader comes after the retriever and is used to find precise answers to specific questions. It's particularly useful when you have a set of documents and you want to extract specific information or answers from them.\n",
        "In summary, the retriever is responsible for document retrieval and ranking, while the reader focuses on understanding the content within those documents to extract answers. Together, these components create a pipeline that allows for efficient and accurate question-answering systems by first narrowing down the search space and then extracting detailed information."
      ],
      "metadata": {
        "id": "dyjBO7KAX9bK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query Input: You provide a query or a question as input.\n",
        "Retriever Stage:\n",
        "Document Retrieval: The query is processed by the retriever, and it retrieves the top-K documents or passages.\n",
        "Embeddings: The retriever generates embeddings for the query and the documents.\n",
        "Similarity Scoring: It calculates similarity scores between the query and the document embeddings.\n",
        "Ranking: The retriever ranks the documents or passages based on their similarity scores, and the top-K are selected as the most relevant.\n",
        "Top-K Selection: The retriever returns the top-K documents as the result.\n",
        "Reader Stage:\n",
        "Document Reading: The top-K documents are passed to the reader component. The reader reads and processes the content of these documents.\n",
        "Answer Extraction: The reader looks for specific answers or information within the documents based on the original query.\n",
        "Output: The output of the reader stage is the answers or specific information extracted from the top-K documents. These answers are returned as the final result of the process.\n"
      ],
      "metadata": {
        "id": "DYLOx9ZpZuDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.document_store.faiss import FAISSDocumentStore\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from haystack.schema import Document\n",
        "\n",
        "# Initialize the FAISS document store\n",
        "document_store = FAISSDocumentStore(sql_url=\"sqlite:///my_faiss_store.db\", index=\"hnsw_flat\")\n",
        "\n",
        "# Initialize a SentenceTransformer model\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Generate embeddings for your data and store them in FAISS\n",
        "for entry in data_to_index:\n",
        "    content = entry[\"data\"]  # Extract the text content\n",
        "    embedding = model.encode(content)  # Generate the embedding\n",
        "\n",
        "    # Create a FAISSDocument with 'content' and 'meta' fields\n",
        "    document = Document(content=content, meta={\"docs\": entry[\"docs\"], \"indexes\": entry[\"indexes\"]})\n",
        "\n",
        "    # Store the document with the embedding in FAISS\n",
        "    document_store.write_documents([document], index=\"hnsw_flat\", update_existing=True)\n"
      ],
      "metadata": {
        "id": "R4QZPaqXxF28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.document_store.faiss import FAISSDocumentStore\n",
        "\n",
        "# Initialize the FAISS document store\n",
        "document_store = FAISSDocumentStore(sql_url=\"sqlite:///my_faiss_store.db\", index=\"hnsw_flat\")\n",
        "\n",
        "# Your dataset\n",
        "data_to_index = [\n",
        "    {\"data\": \"Text from document 1, section A\", \"docs\": \"document1\", \"indexes\": \"A\"},\n",
        "    {\"data\": \"Text from document 1, section B\", \"docs\": \"document1\", \"indexes\": \"B\"},\n",
        "    {\"data\": \"Text from document 2, section A\", \"docs\": \"document2\", \"indexes\": \"A\"},\n",
        "    # Add more data entries as needed\n",
        "]\n",
        "\n",
        "# Store the data in FAISS\n",
        "for entry in data_to_index:\n",
        "    document_store.write_documents([{\"text\": entry[\"data\"], \"meta\": {\"docs\": entry[\"docs\"], \"indexes\": entry[\"indexes\"]}],\n",
        "                                   update_existing=True)\n"
      ],
      "metadata": {
        "id": "pf1x5Rr2yjb9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}