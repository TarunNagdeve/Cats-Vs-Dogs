{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3BjMBn6VF60C7scKl4v2c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TarunNagdeve/Cats-Vs-Dogs/blob/master/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = \"bert-base-uncased\"  # You can choose a different BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = TFBertModel.from_pretrained(model_name)\n",
        "\n",
        "# List of text items\n",
        "text_list = [\"Text 1\", \"Text 2\", \"Text 3\", ...]  # Replace with your text data\n",
        "\n",
        "# Initialize an empty list to store embeddings\n",
        "embeddings_list = []\n",
        "\n",
        "# Loop through each text item in the list\n",
        "for text in text_list:\n",
        "    # Tokenize and encode the text\n",
        "    input_tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "    # Get the BERT embeddings for the text\n",
        "    input_embeddings = model(input_tokens[\"input_ids\"])[0]\n",
        "\n",
        "    # Append the embeddings to the list\n",
        "    embeddings_list.append(input_embeddings)\n",
        "\n",
        "# Convert the embeddings list to a NumPy array\n",
        "embeddings_array = tf.concat(embeddings_list, axis=0).numpy()\n",
        "\n",
        "# Print the embeddings array\n",
        "print(embeddings_array)\n"
      ],
      "metadata": {
        "id": "rRSKalmqpBHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample list of texts (replace with your own)\n",
        "text_list = [\"Text 1\", \"Text 2\", \"Text 3\", \"Text 4\"]\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to create BERT embeddings for a list of texts\n",
        "def create_bert_embeddings(text_list, model, tokenizer):\n",
        "    embeddings = []\n",
        "    for text in text_list:\n",
        "        input_tokens = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "        input_embeddings = model(**input_tokens)['last_hidden_state']\n",
        "        mean_embedding = torch.mean(input_embeddings, dim=1).detach().numpy()\n",
        "        embeddings.append(mean_embedding)\n",
        "    return embeddings\n",
        "\n",
        "# Save the BERT embeddings to a CSV file\n",
        "embeddings = create_bert_embeddings(text_list, model, tokenizer)\n",
        "df = pd.DataFrame({'Text': text_list, 'Embeddings': embeddings})\n",
        "df.to_csv('embeddings.csv', index=False)\n",
        "\n",
        "# Generate BERT embeddings for input text\n",
        "input_text = \"Input text goes here\"\n",
        "input_tokens = tokenizer(input_text, padding=True, truncation=True, return_tensors='pt')\n",
        "input_embedding = torch.mean(model(**input_tokens)['last_hidden_state'], dim=1).detach().numpy()\n",
        "\n",
        "# Load the saved embeddings from the CSV file\n",
        "loaded_df = pd.read_csv('embeddings.csv')\n",
        "\n",
        "# Calculate cosine similarity between input and saved embeddings\n",
        "similarities = cosine_similarity([input_embedding], loaded_df['Embeddings'].tolist())\n",
        "\n",
        "# Find the closest text based on cosine similarity\n",
        "closest_text_index = np.argmax(similarities)\n",
        "closest_text = loaded_df['Text'][closest_text_index]\n",
        "\n",
        "print(f\"Input text: {input_text}\")\n",
        "print(f\"Closest text: {closest_text}\")\n"
      ],
      "metadata": {
        "id": "lqV1JKn-TIGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B5ojqenkW5Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding Generation and Storage:\n",
        "Preprocess your text dataset.\n",
        "Use your pre-trained model to generate text embeddings for each text in the dataset.\n",
        "Store these embeddings as JSON objects where each key represents the unique identifier for a text (e.g., document ID or index), and the corresponding value is the embedding vector.\n",
        "Input Preprocessing:\n",
        "When a new input text is provided, preprocess it similarly as you did for the dataset.\n",
        "Cosine Similarity Search:\n",
        "When performing a similarity search for a new input text, generate the embedding vector for the input text.\n",
        "Retrieve the saved embeddings from your JSON file for all texts in the dataset.\n",
        "Cosine Similarity Calculation:\n",
        "Calculate the cosine similarity between the embedding of the input text and the embeddings of all texts in the dataset.\n",
        "Rank the texts based on their similarity scores.\n",
        "Result Presentation:\n",
        "Return the texts with the highest cosine similarity scores as the most similar texts to the input."
      ],
      "metadata": {
        "id": "GiQFQduOeIGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample dataframe\n",
        "data = {'SingleName': ['Name1', 'Name2', 'Name3'],\n",
        "        'ListNames': [['Name1', 'Name2', 'Name3', 'Name4', 'Name5'],\n",
        "                      ['Name2', 'Name3', 'Name1', 'Name4', 'Name5'],\n",
        "                      ['Name4', 'Name5', 'Name2', 'Name1', 'Name3']]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Count how many times the name in 'SingleName' appears at index 0 in 'ListNames'\n",
        "count = df.apply(lambda row: row['ListNames'].count(row['SingleName']) if row['SingleName'] == row['ListNames'][0] else 0, axis=1)\n",
        "\n",
        "# Add the count as a new column in the dataframe\n",
        "df['Count'] = count\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "CLZlRDtEhNFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample dataframe\n",
        "data = {'SingleName': ['Name1', 'Name2', 'Name3'],\n",
        "        'ListNames': [['Name1', 'Name2', 'Name3', 'Name4', 'Name5'],\n",
        "                      ['Name2', 'Name3', 'Name1', 'Name4', 'Name5'],\n",
        "                      ['Name4', 'Name5', 'Name2', 'Name1', 'Name3']]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Count how many times the name in 'SingleName' appears in 'ListNames'\n",
        "def count_name_occurrences(row):\n",
        "    return row['ListNames'].count(row['SingleName'])\n",
        "\n",
        "df['Count'] = df.apply(count_name_occurrences, axis=1)\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "lZJY8iEhk4e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Define your long sentence\n",
        "long_sentence = \"Your long sentence goes here. It has many tokens and is longer than BERT's maximum limit.\"\n",
        "\n",
        "# Load the BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize the long sentence\n",
        "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(long_sentence)))\n",
        "\n",
        "# Set the maximum token limit for BERT (e.g., 512 tokens for BERT)\n",
        "max_token_limit = 512\n",
        "\n",
        "# Initialize variables\n",
        "embeddings = []\n",
        "\n",
        "# Split the tokens into overlapping chunks\n",
        "for i in range(0, len(tokens), max_token_limit - 50):  # -50 for token overlap\n",
        "    chunk = tokens[i:i + max_token_limit]\n",
        "\n",
        "    # Convert the chunk back to text\n",
        "    chunk_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(chunk))\n",
        "\n",
        "    # Encode the chunk as input to the BERT model\n",
        "    input_ids = tokenizer.encode(chunk_text, add_special_tokens=True, max_length=max_token_limit, truncation=True, padding='max_length', return_tensors='pt')\n",
        "\n",
        "    # Get the BERT embeddings for the chunk\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids)\n",
        "\n",
        "    # Append the chunk's embeddings\n",
        "    embeddings.append(output.last_hidden_state)\n",
        "\n",
        "# Concatenate or aggregate the embeddings, e.g., by taking the mean\n",
        "final_embeddings = torch.cat(embeddings, dim=1).mean(dim=1)\n",
        "\n",
        "# The 'final_embeddings' now contains the embeddings for the entire long sentence\n",
        "\n",
        "# Use 'final_embeddings' for your downstream NLP tasks\n"
      ],
      "metadata": {
        "id": "oCWShakTQc6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Define your long sentence\n",
        "long_sentence = \"Your long sentence goes here. It has many tokens and is longer than BERT's maximum limit.\"\n",
        "\n",
        "# Load the BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize the long sentence\n",
        "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(long_sentence)))\n",
        "\n",
        "# Set the maximum token limit for BERT (e.g., 512 tokens for BERT)\n",
        "max_token_limit = 512\n",
        "\n",
        "# Initialize variables\n",
        "embeddings = []\n",
        "\n",
        "# Split the tokens into overlapping chunks\n",
        "for i in range(0, len(tokens), max_token_limit - 50):  # -50 for token overlap\n",
        "    chunk = tokens[i:i + max_token_limit]\n",
        "\n",
        "    # Convert the chunk back to text\n",
        "    chunk_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(chunk))\n",
        "\n",
        "    # Encode the chunk as input to the BERT model\n",
        "    input_ids = tokenizer.encode(chunk_text, add_special_tokens=True, max_length=max_token_limit, truncation=True, padding='max_length', return_tensors='pt')\n",
        "\n",
        "    # Get the BERT embeddings for the chunk\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids)\n",
        "\n",
        "    # Append the chunk's embeddings\n",
        "    embeddings.append(output.last_hidden_state)\n",
        "\n",
        "# Concatenate or aggregate the embeddings, e.g., by taking the mean\n",
        "final_embeddings = torch.cat(embeddings, dim=1).mean(dim=1)\n",
        "\n",
        "# The 'final_embeddings' now contains the embeddings for the entire long sentence\n",
        "\n",
        "# Use 'final_embeddings' for your downstream NLP tasks\n"
      ],
      "metadata": {
        "id": "3ITaVCuFRZfG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}