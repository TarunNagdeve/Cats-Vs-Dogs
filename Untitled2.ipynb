{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPi5LIVivK9awgxpw/BysqR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TarunNagdeve/Cats-Vs-Dogs/blob/master/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = \"bert-base-uncased\"  # You can choose a different BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = TFBertModel.from_pretrained(model_name)\n",
        "\n",
        "# List of text items\n",
        "text_list = [\"Text 1\", \"Text 2\", \"Text 3\", ...]  # Replace with your text data\n",
        "\n",
        "# Initialize an empty list to store embeddings\n",
        "embeddings_list = []\n",
        "\n",
        "# Loop through each text item in the list\n",
        "for text in text_list:\n",
        "    # Tokenize and encode the text\n",
        "    input_tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "    # Get the BERT embeddings for the text\n",
        "    input_embeddings = model(input_tokens[\"input_ids\"])[0]\n",
        "\n",
        "    # Append the embeddings to the list\n",
        "    embeddings_list.append(input_embeddings)\n",
        "\n",
        "# Convert the embeddings list to a NumPy array\n",
        "embeddings_array = tf.concat(embeddings_list, axis=0).numpy()\n",
        "\n",
        "# Print the embeddings array\n",
        "print(embeddings_array)\n"
      ],
      "metadata": {
        "id": "rRSKalmqpBHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample list of texts (replace with your own)\n",
        "text_list = [\"Text 1\", \"Text 2\", \"Text 3\", \"Text 4\"]\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to create BERT embeddings for a list of texts\n",
        "def create_bert_embeddings(text_list, model, tokenizer):\n",
        "    embeddings = []\n",
        "    for text in text_list:\n",
        "        input_tokens = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "        input_embeddings = model(**input_tokens)['last_hidden_state']\n",
        "        mean_embedding = torch.mean(input_embeddings, dim=1).detach().numpy()\n",
        "        embeddings.append(mean_embedding)\n",
        "    return embeddings\n",
        "\n",
        "# Save the BERT embeddings to a CSV file\n",
        "embeddings = create_bert_embeddings(text_list, model, tokenizer)\n",
        "df = pd.DataFrame({'Text': text_list, 'Embeddings': embeddings})\n",
        "df.to_csv('embeddings.csv', index=False)\n",
        "\n",
        "# Generate BERT embeddings for input text\n",
        "input_text = \"Input text goes here\"\n",
        "input_tokens = tokenizer(input_text, padding=True, truncation=True, return_tensors='pt')\n",
        "input_embedding = torch.mean(model(**input_tokens)['last_hidden_state'], dim=1).detach().numpy()\n",
        "\n",
        "# Load the saved embeddings from the CSV file\n",
        "loaded_df = pd.read_csv('embeddings.csv')\n",
        "\n",
        "# Calculate cosine similarity between input and saved embeddings\n",
        "similarities = cosine_similarity([input_embedding], loaded_df['Embeddings'].tolist())\n",
        "\n",
        "# Find the closest text based on cosine similarity\n",
        "closest_text_index = np.argmax(similarities)\n",
        "closest_text = loaded_df['Text'][closest_text_index]\n",
        "\n",
        "print(f\"Input text: {input_text}\")\n",
        "print(f\"Closest text: {closest_text}\")\n"
      ],
      "metadata": {
        "id": "lqV1JKn-TIGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B5ojqenkW5Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding Generation and Storage:\n",
        "Preprocess your text dataset.\n",
        "Use your pre-trained model to generate text embeddings for each text in the dataset.\n",
        "Store these embeddings as JSON objects where each key represents the unique identifier for a text (e.g., document ID or index), and the corresponding value is the embedding vector.\n",
        "Input Preprocessing:\n",
        "When a new input text is provided, preprocess it similarly as you did for the dataset.\n",
        "Cosine Similarity Search:\n",
        "When performing a similarity search for a new input text, generate the embedding vector for the input text.\n",
        "Retrieve the saved embeddings from your JSON file for all texts in the dataset.\n",
        "Cosine Similarity Calculation:\n",
        "Calculate the cosine similarity between the embedding of the input text and the embeddings of all texts in the dataset.\n",
        "Rank the texts based on their similarity scores.\n",
        "Result Presentation:\n",
        "Return the texts with the highest cosine similarity scores as the most similar texts to the input."
      ],
      "metadata": {
        "id": "GiQFQduOeIGT"
      }
    }
  ]
}